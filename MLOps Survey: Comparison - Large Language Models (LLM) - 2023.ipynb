{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIldPOcAN6nbpbhs4l2S/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmind/mlops-survey-chatgpt-2023/blob/main/MLOps%20Survey%3A%20Comparison%20-%20Large%20Language%20Models%20(LLM)%20-%202023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INPUTS"
      ],
      "metadata": {
        "id": "G_S3FtvK0FTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A0CcAP41T3l"
      },
      "outputs": [],
      "source": [
        "RESULTS = '''What is your position/title at your company?,How big is your organization? (number of employees),Are you using LLM at in your organization?,What is your use case/use cases?,Have you integrated or built any internal tools to support LLMs in your org? If so what?,What are some of the main challenges you have encountered thus far when building with LLMs,What are your main concerns with using LLMs in production?,how are you using LLMs?,What tools are you using with LLMs?,What areas are you most interested in learning more about?,How do you deal with reliability of output?,Any stories you have that are worth sharing about working with LLMs?,Any questions you have for the community about LLM in production?,What is the main reason for not using LLMs in your org?,What are some key questions you face when it comes to using LLM in prod?,have you tried LLMs for different use cases in your org?,\"If yes, why did it not work out?\",Any stories you have that are worth sharing about working with LLMs?,Any questions you have for the community about LLM in production?\n",
        ",,,,,,,,,,,,,,,,,,\n",
        "Data scientist,\"1,000+\",No,,,,,,,,,,,Production takes investment,Cost to maintain the service,Yes,Date quality,N/a,\n",
        "ML Engineer,50-500,Yes,Text summarisation,No,\"Current infrastructure not designed for such massive models, having to implement workarounds for quick fixes\",Cost,\"Open source model (GPT-J, etc)\",Seldon,Inferenece for LLMs,,,,,,,,,\n",
        "Founder,1-10,Yes,Data annotation; summarization; search,\"https://chat.mantisnlp.com, internal annotation tools\",\"out of date info, hallucinations, cost, difficulty in deploying on own infrastructure\",they could serve nonsense or out of date information,Open AI API,,Working with OSS LLMs,we provide links to the context from which answers have been drawn.,,,,,,,,\n",
        "Owner,1-10,Yes,Content production and brainstorming,Not yet,not yet,cost of openai API,Open AI API,\"Looking at Pinecone, weaviate, langchain, hugging face\",Embeddings,temperature = 0,,,,,,,,\n",
        "Senior Principal Scientist,\"1,000+\",Yes,Healthcare,\"yes, confidential\",reliability,cost of openai API,In house model,,Working with OSS LLMs,,,,,,,,,\n",
        "Head of Machine Learning,50-500,No,,,,,,,,,,,It's only just now becoming feasible with the ChatGPT API. Before it was too expensive and quality was not good enough.,How to make them reliable.,\"We are still exploring, eg for support in diagnosis of plant diseases.\",So far it's not reliable enough and we don't have spent enough time preparing special content for it. Like rules it should use during diagnosis. The data ChatGPT was trained on seems a bit hit and miss for us.,,\"Anyone have some experience with getting better reliability out of ChatGPT? Anyone succeeded in fine-tuning and getting better quality than ChatGPT? Thinking of llama for example.\n",
        "How can you limit ChatGPT so users will not ask it totally random stuff?\"\n",
        "Senior Machine Learning Engineer,\"1,000+\",Yes,\"Search, classification, NER\",Just a bunch of shitty scripts I need to refactor in the future.,\"Size in all regards. Also training time, response time and that multi gpu debugging is weird.\",\"Whether the cost involved for us justifies the gains we get, considering we mainly make money from ads and not a really good product.\",Other model provider API,\"Hugginface, sagemaker, openapi api, tensorflow.\",Working with OSS LLMs,\"Ill refer to the picture of the xkcd guy \"\"just stir the model until the numbers look right\"\".\",\"I am not sure what ranks as a LLM. Anything bert and beyond? Anyway, we once used setfit for a production task but found that performance was really tricky to debug. The proof of concept was fine. Production was a complete mess. To this day I dont know why- underlying hardware?\",I mainly wonder how everyone is monitoring these. ALSO. For my company GDPR is a big deal. My DPO shoots down our ideas more often than not or limits it to using the last three months of data. How do other people deal with this?,,,,,,\n",
        "Head of Product,10-50,No,,,,,,,,,,,No applications,ROI of more expensive models,Yes,No ROI,N/A,Interested in results of survey\n",
        ",50-500,Yes,,oi,lik,il,Open AI API,loi,Working with OSS LLMs,lo,lo,lo,,,,,,\n",
        "Senior Data Engineer,50-500,Yes,\"text embeddings for search, text generation to help the business people do their job\",\"Yes, we're using OpenAI for a couple of internal tools.\",,Cost. But our issues with productionisation is more general than just for LLMs,Open AI API,streamlit,Inferenece for LLMs,\"All internal tools, reliability is not a major concern for now\",,,,,,,,\n",
        "Machine learning engineer,\"1,000+\",Yes,\"Natural Language interface, domain specific content creation\",,Prompt debugging! We need a new form of debugger.,They are erratic,Open AI API,,Working with OSS LLMs,We have control systems,,,,,,,,\n",
        "Senior Data Scientist,\"1,000+\",Yes,Chatbots and Text classification,One of our use cases is to build text classification models for each of our customers (we are a B2B company). Achieved this using LLMs and some engineering,,,\"Open source model (GPT-J, etc)\",,Embeddings,,,,,,,,,\n",
        "ML Engineer,50-500,No,,,,,,,,,,,,,,,,\n",
        "Snr MLOps Engineer,50-500,Yes,Adapters,\"Yes, training setup as well as inference setup\",Latency and deployment time,Resources usage along with latency and deployment time,\"Open source model (GPT-J, etc)\",,Inferenece for LLMs,Assessment at evaluation,,,,,,,,\n",
        ",\"1,000+\",Yes,,,,,,,Inferenece for LLMs,,,,,,,,,\n",
        "Senior ML engineer,50-500,No,,,,,,,,,,,\"Two main reasons: 1. Very compute & power hungry 2. Cannot be relied upon as it will very confidently return incorrect information or in ‚Äúexistent facts‚Äù which is problematic for use in my industry.\n",
        "\n",
        "Eg it took us less than 3mn to get BioGPT to tell us vaccines cause autism\",\"What uses cases does it have for our business? how much gain do we get from using an LLM vs a simpler language model where we can control fact and outputs? How much will it cost (to deploy, and run)\",We abandoned the idea pretty quickly,We got terrible suggestions from the LLM that were false information and would be detrimental to the business,Ask BioGPT about vaccines and autism and it‚Äôll confidently tell you that vaccines cause autism spectrum disorders in a small percentage of children (yikes),How do you force an LLM to only return factual information ?\n",
        "Machine Learning Engineer,50-500,No,,,,,,,,,,,\"We use medium-sized language models (e.g. RoBERTa-base) as the backbone for our deployed NLP models because they can perform inference on CPU, which is cheap. We don't use LLMs (e.g. ChatGPT, GPT-3) because the APIs are expensive, and fine-tuning them to perform NLP tasks other than language generation (e.g. NER, open-set intent classification) is not straightforward.\",How can we ensure the same version of the model is being used? How can we supply a random seed to get deterministic results? How can we minimize LLM API usage costs? How can we fine-tune a LLM on a niche task without having to replicate the weights of the LLM? How can we deploy our own copy of an open-source LLM without breaking the bank?,Yes,\"We evaluated a LLM (GPT-3) against RoBERTa fine-tuned on our small task-specific dataset, and our fine-tuned model got much higher accuracy than GPT-3 did.\",None,\n",
        ",\"1,000+\",No,,,,,,,,,,,Not much usecases for us,,,,,\n",
        "Founding Data Scientist,1-10,Yes,\"Summarization, topic extraction, root-cause analysis\",\"Not much, just some prompt storage and an automated retries to deal with different failure cases.\",\"Hallucinations are a big one. In general, problems are hard to debug without careful manual review, so we're only using them when the amount of results generated is small enough to review by hand.\",\"Reliability, cost, injecting domain-specific language and understanding into them\",Open AI API,,Embeddings,\"Automated retries for some simple & obvious failures, manual review for everything else\",,Where are others drawing the line between training models locally and using LLMs? Have others had success with using LLMs to bootstrap training data for smaller local models?,,,,,,\n",
        "CTO,1-10,Yes,Marketing,Not yet,Still developing,Cost,Open AI API,,Fine tunning LLMS,,,,,,,,,\n",
        "Machine Learning Engineer,\"500-1,000\",No,,,,,,,,,,,Not applicable to the domain,Not yet,,,,Can you actually compete with the giants for a good language model?\n",
        "Head of AI,\"1,000+\",Yes,Scanning large text corpora using an LLM and some cases with text summary for now,Not (yet).,It‚Äôs a mix of taming a beast and using alchemy,Consistency of the output quality,Other model provider API,./.,Working with OSS LLMs,HITL,They take every Input equally serious which makes for some great laughs‚Ä¶,,,,,,,\n",
        "Product,\"1,000+\",No,,,,,,,,,,,Production LLM lifecycle and use cases not clear,Data governance,no,,,\n",
        "Lead Data Scientist,50-500,No,,,,,,,,,,,Cost / right use-case,,,,,\n",
        "Co-founder,1-10,Yes,AI for e-commerce,No,Hardware requirements and AI accelerators compatibility,Inference scalability,\"Open source model (GPT-J, etc)\",\"PyTorch, AWS SageMaker, OpenAI's API, Midjourney, CLIP/BLIP/etc models\",Inferenece for LLMs,,,\"Any opinions regarding the newer AI accelerator chips? Many people fighting for GPUs, but not necessarily mentioning AWS Inferentia 2 chips, TPUs, Habana Gaudi, etc.\",,,,,,\n",
        "MLE,50-500,Yes,,dbt pre-processing to get the text right for the prompts and test it,\"explainability, expense, embeddings\",see above,Open AI API,metaflow,Embeddings,,,,,,,,,\n",
        "Snr MLE,50-500,No,,,,,,,,,,,Exploration stage,Putting company data in the right structure and updating the model with it,yes,getting fine tune right and LLMops pipeline,,Is it cheaper to host your own LLM on AWS or just use OpenAI services? With respect to scaling and potential user base of 5000\n",
        "Director of Artificial Intelligence,\"1,000+\",Yes,\"Novelty - ChatGPT is being explored to optimize code, critique written works, etc.\",No,N/A,Complacency and atrophy of critical thinking,\"Open source model (GPT-J, etc)\",Microsoft Azure OpenAI services,Fine tunning LLMS,Educate others on the limitations and proper use.,\"One of our citizen data scientists now (figuratively) \"\"worships\"\" ChatGPT Gods given its capabilities. LOL.\",None at the moment given our priorities are mostly with computer vision and time-series data analysis.,,,,,,\n",
        "Data Scientist,50-500,Yes,\"Summarisation, Email generation, Information retrieval\",Modular python library for interacting with multiple LLM providers/APIs and a standardized prompt generation framework.,Consistency of formatting in freeform LLM text generation. Subtleties in prompt engineering.,\"Detecting truthfulness, hallucination. Output validation. Risk of monopolisation and centralised access to LLM as a service\",Other model provider API,Python,Working with OSS LLMs,,,,,,,,,\n",
        "Co-founder and CTO,1-10,Yes,Prompt autogeneration and synthetic data generation,,Reproducibility,Hallucination and latency,Open AI API,HoneyHive,Fine tunning LLMS,\"Multi-responses shown to user side by side, ghost writer style dropout\",,How are you thinking about tracking user feedback in production?,,,,,,\n",
        ",50-500,Yes,,,,,,,Working with OSS LLMs,,,,,,,,,\n",
        "Senior Software Enginner,50-500,No,,,,,,,,,,,No business need yet,,,,,\n",
        "Co-founder,1-10,No,,,,,,,,,,,We're learning,,,,,\n",
        "ML Developer,\"500-1,000\",Yes,Question Answering,yes,latency,\"latency, factuality\",\"Open source model (GPT-J, etc)\",,Inferenece for LLMs,,,,,,,,,\n",
        ",1-10,No,,,,,,,,,,,\"Cost, Latency, Accuracy\",\"Cost can be high, hallucinations\",,,,\n",
        "data scientist,\"500-1,000\",No,,,,,,,,,,,not really in our domain,N/A,no - closest we've got to dealing with language is fuzzy lookups,N/A,Nope - but I'm excited to learn more,\n",
        "Senior Machine Learning Engineer,10-50,Yes,Question generation for accountants,built internal,reducing latency,not knowing the data used for the initial training,In house model,\"TensorFlow, TFX\",Inferenece for LLMs,yes,,,,,,,,\n",
        "Founder,1-10,Yes,Zero-shot/Few Shot classification for data enrichment,\"Internally, we use BERT to create embeddings, then train a smaller model on customer data.\",Infrastructure costs.,\"Is what I;m building getting obsolete in 3 months,\",In house model,In house Bert (from Huggingface) + Open AI,Fine tunning LLMS,I am stuck with this right now - can I integrate these into the UI for AI Hero.,ChatGPT API output is different every time for the same input - makes it hard to create a reliable service.,How are you managing training/deployment of these?,,,,,,\n",
        "SRE,\"1,000+\",Yes,analyze logs,no,Analyzing results,security,In house model,in house built tool,Fine tunning LLMS,Very manual so far.,confidential,no,,,,,,\n",
        "\"Cofounder (normally product manager, but also wearer of many hats)\",1-10,No,,,,,,,,,,,\"1. gcm diagflow is good for complete basic dialogs, 2. halucinations 3.a. how to make answers be focused on what my company is doing rather than general answers, 3.b. how to keep that evolving as we learn more about the customers 3.c. how to prevent a practical joker who's conversing with my llm to inject some practical joke into it 4. if im relying on openai-s llm, i need to always get a reply what happens if they're busy, whats the proverbial 'failover'\",how to get it to use my company's specific knowledge,not thoroughly enough yet,still working on it,openai - even with monthly subscription it is not predictable output,\"do you use it for chatbots, how do you put guardrails on it for conversations that are going off track or how do you prevent practical jokes if you're re-incorporating the knowledge back into the llm\"\n",
        "Data Architect,\"1,000+\",Yes,Search and chat on proprietary data,No,\"Automation amd scaling of transfer learning, tuning and reinforcement learning\",Scaling,Open AI API,\"Lang chain, k8s, gcp\",Learning from new/private data,Not a concern. In-house,No,\"Best automated/scalable pipeline for tuning,, transfer learning (augmentation), reinforcement feedback\",,,,,,\n",
        "Senior software engineer,\"1,000+\",No,,,,,,,,,,,No need atm,Reliability,,,,\n",
        "consultant,1-10,No,,,,,,,,,,,we'll be using at a startup I'm part of,cost and fit for purpose. Actual market validation,yes. I've used them for classification,they are in prod for that use case,,\n",
        "CTO,50-500,Yes,\"Product (Software) recommendations, Internal Q&A, Marketing content, and Customer support\",\"Langchain, OpenAI Playground, ChatGPT, PromptLayer, Weaviate\",Rapidly changing APIs,Hallucinations misleading customers,Open AI API,PromptLayer,Embeddings,\"Pre/post-processing, constitutional / self-auditing, customer feedback thumbs up/down, custom embedding matrix\",,How might you avoid siloing all of your embeddings when different teams need different tools without building it all in house yet all would benefit from the combined collection?,,,,,,\n",
        "Data scientist / co-founder,1-10,Yes,Information retrieval and text generation.,\"We have some output validation methods and templates, basically regex matching prompts.\",Input length limitation and having to work around it with non-optimal methods.,Hallucinations,Open AI API,\"Langchain, Huggingface, Polars\",Fine tunning LLMS,We implement checking with less stochastic tools.,Content language matters a lot!,,,,,,,\n",
        "Analytics engineer,50-500,Yes,Chatbot,In POC at the moment,Vectorizing CRUD and ci / CD,To get better at semantic search,Open AI API,\"Langchain, pinecone, slack and aws\",Embeddings,,,\"How to better deploy python apps using aws and operationalizing it.\n",
        "How to better manage vector indexes.\",,,,,,\n",
        "\"Director, Innovation\",50-500,No,,,,,,,,,,,Data privacy concerns,Difficulty or ease of maintenance; costs; data privacy,Not yet,na,None yet,Are there any standards/best practices to deal with data privacy?\n",
        "ML & MLOps practice lead,50-500,Yes,\"Text classification, text generation\",,\"Compute performance, fine tuning challenges, explainability\",,\"Open source model (GPT-J, etc)\",,Inferenece for LLMs,,,,,,,,,\n",
        "Founder,1-10,Yes,Social media idea generation,No just used langchain and OpenAI,Prompt engineering; handling very large documents,Bad data/completions; runtime errors parsing completions (we use regex to parse each line ),Open AI API,LangChain,Fine tunning LLMS,We don‚Äôt do anything right now,,,,,,,,\n",
        "CTO,1-10,Yes,Semantic data extraction and corporate governance,\"I'm unable to utilise LLM's currently due to data security restrictions, however as a OpenAI beta tester, around 2 years ago I created a business Operational Delivery Framework for a ¬£500M nuclear project. I then transferred the learning framework to ¬£5B project.\",Environment dependency conflicts,Safe and ethical usage,Open AI API,Use case dependent,Fine tunning LLMS,\"AI is replaced by HI(Human) for final check, always.\",\"Yes, the ODF I created was regarded as best in class by regulators and has been suggested to be implemented as new gold standard. I used GPT3 to advise on the design of the framework, and to write all the necessary code to create it.\",\"Nope, no questions, just a great big thankyou, and a virtual high five for all the awesome peeps üôè\",,,,,,\n",
        "MLOps Engineer,50-500,No,,,,,,,,,,,Still evaluating infrastructure requirements and ways to make it traceable if things go wrong,\"Retraining, tackle hallucinations and avoid wrong results\",No,,,\n",
        "Junior developer,1-10,Yes,Creating a chat bot to query data efficiently for managers,Not yet but its in production. Using langchain.,\"The main challenges are using the different tools from langchain to optimise for the use case. There are a lot of things to consider. Also, we use ts and angular front end so not having all the tools that are available in python required us to have python as the backend now.\",It‚Äôs how useful/valuable it will actually be for the client.,Open AI API,Langchain/ts/csharpe/angular.,Fine tunning LLMS,We havent gotten to that yet.,\"They are amazing. But as mentioned, a chatbot alone is probably not enough to be useful, hence building out other capabilities will become important.\",,,,,,,\n",
        "deep learning engineer,\"1,000+\",Yes,extract information from image and text of medical domain,no,deploy models into production,hight training & inference cost,\"Open source model (GPT-J, etc)\",\"transformers, triton inference server\",Working with OSS LLMs,,,,,,,,,\n",
        "Co-Founder,10-50,No,,,,,,,,,,,Lack of use cases that require so much sophistication,\"cost of maintenance, lack of institutional knowledge\",No,,\"Have previously tried using them, however, in many cases, LLMs aren't all that reliable. I tried to use it as a chatbot for a client but it didn't work as expected in many cases which is why didn't end up using it in production.\",None\n",
        "Intern,\"1,000+\",No,,,,,,,,,,,Because I am in education sector.,when you use LLM in production it can fail any time,NO,,,\n",
        "CTO,1-10,Yes,Coding,VSCode Extension,Work with 3 AI providers at the same time,The wrong respond from the model,Open AI API,Code GPT https://codegpt.co,Embeddings,Now it is just responds tickets in github,\"The extension hace more than 200,000 downloads and it work great!\",,,,,,,\n",
        "Software Development Engineer,\"1,000+\",Yes,\"(Not in my org, at work I don‚Äôt do this, it‚Äôs personal) Cover letter generator based on resume and job description\",,Embedding with appropriate data effectively,Being dependent on one company for the API so far. Having models of equivalent performance whose weights were public would be a great relief,Open AI API,\"Gpt index, langchain\",Embeddings,\"Prompt engineering, allow user to be specific, and of course re-generate\",,,,,,,,\n",
        "Data Science Team Lead,10-50,Yes,Topic sentiment evaluation on text,,Managing training data labeling,explainability,\"Open source model (GPT-J, etc)\",AWS Sagemaker,Fine tunning LLMS,Test each new model candidate on test set,,,,,,,,\n",
        "Sr. ML Eng,50-500,No,,,,,,,,,,,inference cost vs. impact on revenue,how to reduce inference cost,,,,\n",
        "Senior AI Expert,50-500,Yes,Medical Diagnostics Support,\"No, we rely on open source tools\",\"Explainability, Testing, Getting ground truth data\",Trustworthiness,\"Open source model (GPT-J, etc)\",\"Milvus DB, FastAPI, Hugginface, Pytorch, etc.\",Working with OSS LLMs,We try to make the output as transparent as we can (using explainability techniques) and not relying upon LLM for the end decision (these are used as a support tool),,,,,,,,\n",
        "Data Science Engineer,50-500,Yes,Assistant chat bot,\"Langchain. Will serve the bot from a flask server, being set up for the bot, into our existing java platform\",\"Langchain LLM Parse error. Turbo not using multiple tools for some reason, when davinci-003 did\",\"Need HIPAA compliance, so trying to get on Azure. Worried about the contract falling through\",Open AI API,Langchain. All my agent‚Äôs tools are custom-built. I hit our internal APIs with them,I want deep knowledge on how to make the ReAct pattern work in various edge cases,\"Will have friendly internal users using it first in the course of serving customers, and giving us per-message feedback\",davinci-003 hallucinating using an existing tool without actually using it!,How should I split my initial prompt up between turbo‚Äôs System prompt and the first Human prompt I give it?,,,,,,\n",
        "Head of Data and ML,10-50,No,,,,,,,,,,,\"No Use case, being a fintech startup, besides helping with processing documents\",why waste the money,no,,,\n",
        "ML/MLOps Engineer,\"500-1,000\",No,,,,,,,,,,,No clear use case,\"Running them, fine tuning, cost\",Nope,,,\n",
        "\"Data Scientist, HappyFresh\",\"500-1,000\",No,,,,,,,,,,,Not,,,,,\n",
        "Data Scientist,10-50,Yes,Automation of report writing.,\"Yes.\n",
        "Responsible for the design, building, testing and deployment of a GPT-3.5 application that processes numerical data and free-response text in tabular form to generates commentary and analysis for a visual report. Researchers are only required to upload a zip folder(1-click).\n",
        "Technologies: AWS lambda, Docker (ECR/ECS), GPT3.5, PyTorch.\",\"control of the output for our usecase. Reports need to understand the client's intent and take information from briefing calls and key erquriements. These can help guide the output, but now always.\",Thta it will hallucinate something that we won;t pick up in the report editing phase,Open AI API,Torch and Pandas,Prompt Engineering (Riley Goodside),Have editors comb through it,We aim to turn a report around from survey finishing to client inbox in 24 hour. So far we are at 48 hours.,,,,,,,\n",
        "Data Scientist,50-500,Yes,\"LLM in Marketing, Custom data\",HF Inference Endpoints,Cloud Cost,No proper documentation,Other model provider API,,Fine tunning LLMS,,,,,,,,,\n",
        "Senior Data Scientist,\"500-1,000\",No,,,,,,,,,,,\"We don't have a need for generating unstructured data, we work in small languages, our text is very domain specific, LLMs are very compute intensive -> expensive to run and take a lot of work to set up.\",\"Cost, latency, actual performance in the languages / domains we work with (real languages, not programming)\",Nope. Tried some BERT for classification but that hardly counts as an LLM. And I've made models as big as BERT I think.,,,Why are most of the models on Hugging Face based on WordPiece and BPE tokenizers? Give me bytes or give me death\n",
        "Data Scientist,50-500,Yes,chatbot/voice assistant,just used RASA for chatbot creation and haystack for open domain question answering,\"currently used only pre-trained models, biggest challenge was to obtain data for specific domain fine tuning\",it is very important in my opinion to accurately monitor the kind of usage to prevent frauds or malicious intentions,Other model provider API,Haystack,Fine tunning LLMS,\"it depends from specific context and specific use cases, in my experience I rely on validation set and a validation tool that estimates accuracy of answers based on answers manually scrutinized by humans\",not so far,it would be interesting a discussion about how to control the output and in general the kind of usage the system has been exposed,,,,,,\n",
        "Head of Data Science,\"1,000+\",Yes,\"Entity matching, customer service responses (souped up/targetted FAQ).\",yes - high scale server on K8's,K8's caches... TESTING.,\"cost, latency, truthfulnes - but also\n",
        "\n",
        "Maybe management in the sense of versioning and resolving / attributing issues?\n",
        "\n",
        "Copyright and intellectual property liability - who knows what it's been trained on and what it might spit out... What happens if it provides a Disney script for an advert? (I know the answer - nothing good).\n",
        "\n",
        "Safety : what if it tells someone to kill themselves? Or spits out a recipe to make smallpox? (I am being kind of silly here, but... not entirely)\",\"Open source model (GPT-J, etc)\",\"not sure what this means - we use k8's and cloud functions to run them, also interface to case management solutions.\",calibration of llm output / calbration and testing of llms,badly. We do lots of testing but I constantly feel we are on shaky ground.,\"We had a nice time selecting models, proving it would all work and using TSNE and PCA to reduce the dimensionality of embeddings and get compression for our vector lookup system... Very good and satisfying work, we got prizes and praising from all. Then camesome dreadful times making our vector lookup system work under load for long durations in a proper prod environment (where we couldn't touch it to do things like restart it or reload the index). I think this will be a big issue in the future - we need things that are as reliable as oracle for running the inferences, we do not have them - everything is glued together (I am guessing this is not true if you work for Google...)\",,,,,,,\n",
        "Founder,1-10,Yes,We are making tools to monitor and improve the performance of LLMs,No additional tools,,Hallucinations,\"Open source model (GPT-J, etc)\",UpTrain,Embeddings,We are building a tool for that,,How is everyone thinking about fine-tuning LLMs for their use-cases?,,,,,,\n",
        "CEO,1-10,Yes,\"Embeddings, Feature Extraction, Chatbot\",Database connections,Context length limitations,\"Hallucinations, rate limiting,\",Open AI API,\"Langchain, Llama Index, Deep lake, Weaviate\",Fine tunning LLMS,Error handling,,,,,,,,\n",
        "Founder,1-10,Yes,\"Text summarization, code generation\",No,Dealing with vector stores,Keeping agent on track with system (OpenAI‚Äôs ChatGPT) or role,Open AI API,LangChain,Embeddings,N/A,N/A,What are some of the best practices for dealing with LLMs or foundation models?,,,,,,\n",
        "Senior Dara scientist,\"1,000+\",Yes,\"Chatbot, semantic search\",Deploy open source LLM,\"Data privacy, big models\",Fake information,\"Open source model (GPT-J, etc)\",PyTorch huggingaface,Inferenece for LLMs,Generate multiple output,Not really,Chatgpt for domain knowledge,,,,,,\n",
        "ML Engineer,\"1,000+\",Yes,Customer support,We‚Äôve integrated with model store!,Memory requirements during inference,Slow start up times/it‚Äôs very different from regular code,In house model,Hugging Face,Inferenece for LLMs,We‚Äôre doing classification not generation,,Curious to hear about other use cases!,,,,,,\n",
        "Director,1-10,No,,,,,,,,,,,No need yet,I don't need them,\"Yes, too heavy\",Too heavy,Not really,\n",
        "Technical Marketing Manager,10-50,Yes,Content marketing and text drafting,not at the moment in my department,,costs under control and reliability,\"Open source model (GPT-J, etc)\",,Inferenece for LLMs,,,,,,,,,\n",
        "Head of MLOps,\"1,000+\",No,,,,,,,,,,,\"Currently assessing the risk and potential applications, performing POCs.\",\"What are the potential risks, where are the benefits worth the costs for enterprise grade integration, how do we make these things secure, how do we monitor?\",In the process of doing so.,We‚Äôre being thorough.,,Anyone any good resources on analysing the risks and how to monitor LLM based solutions?\n",
        "CDO,\"1,000+\",No,,,,,,,,,,,Wee are exploring the space but don't feel they are ready for production use yeet,\"How to pin them to a domain, how to verify the veracity of teh outcome and how to best chain them with APIs\",We are exploring customer facing and colleague facing applications,Concerns about sticking to a domain and about veracity if the answers,,What use cases do people have? what tooling do they recommend? how do they deal with the halucinations?\n",
        "Owner,1-10,Yes,\"sales pipelines, text generation, data classification and many more\",\"Yes, several\",The need to use Python,\"Cost, accuracy\",Open AI API,Zapier,Finetuning and embeddings,extensive testing,,,,,,,,\n",
        "Agricultue consultant,\"1,000+\",No,,,,,,,,,,,$,$,No,No,No,No\n",
        "Data scientist,1-10,Yes,,Yes,Computing power,,Open AI API,,Fine tunning LLMS,,,,,,,,,\n",
        "Product Owner Data Science,50-500,No,,,,,,,,,,,MLOps and DataScience rather young discipline. We are currently working on an LLM use case though,\"Runtime, Drift detection\",No,,,\n",
        "Team Lead DS & MLE,50-500,No,,,,,,,,,,,Domain specific challenges for what we do. Digging into how we could train LLM to fit the needs.,What is it? There is just the huge gap in understanding about analyzing text vs generating text.,We were using s-BERT in our primary recsys model. Recent modeling efforts have reset to simpler systems that are performing better. Still using it in a 2nd model for a clustering challenge.,,,\n",
        "Co-founder,1-10,Yes,Semantic vector embeddings as a feature into RecSys,Not yet,\"Sparse representations (e.g. tfidf) are easier to work with for our use-cases, because they survive simple aggregation better.\",Explainability of embeddings.,\"Open source model (GPT-J, etc)\",,Embeddings,We have come up with a large list of proxy metrics that we found correlate with performance on our tasks - so we track those.,\"As far as embeddings are concerned, we haven't found a clear advantage of LLMs vs simpler models (tfidf) trained on a relevant dataset. Both require elaborate preprocessing of the embedded documents for example.\",I'd love to connect with others who use LLMs embeddings and talk in more detail :),,,,,,\n",
        "Machine Learning Engineer,50-500,No,,,,,,,,,,,it isn't in focus on my leaders to use LLMs right now,Two key questions I do myself on LLM in prod are: 1 - How is the best strategy to train or fine-tune a LLM ? 2 - How to manage infrastructure costs ?,not yet,.,.,What tools are using to training or fine-tune theses models and a case on deploy a model as an API how has been the issues ?\n",
        "CEO,1-10,No,,,,,,,,,,,\"No current need, but also tooling\",\"Reliability, safety, certainty, hallucinations, etc\",No,,,\n",
        "Principal Cloud Developer,50-500,Yes,\"text resume, make ppts, text quizzes, text to flash cards, video resume etc,etc\",-,-,\"bad output, costs etc,etc\",Open AI API,-,Fine tunning LLMS,-,-,-,,,,,,\n",
        "CPO,1-10,Yes,We help organizations transition Excel reports to Python,\"Yes -- our product, Mito, is a spreadsheet that generates Python code as you edit it. We have integrated the chatgpt api into Mito so users can generate code using the LLM. Then, they can use the Mito spreadsheet to verify that the code it generated is correct.\",We are not yet sure how large enterprises are going to adopt LLMs. Our large enterprise customers are currently not using the Mito AI feature because there is too much uncertainty.,,Open AI API,Just the Mito spreadsheet,Figuring out how to deploy LLMs for enterprise use,We give users a spreadsheet interface to verify the output of the LLM. Spreadsheets are great for understanding changes to data!,,,,,,,,\n",
        "Cloud MLOps Engineer,10-50,Yes,Translate french text to english,Yes we have used opus-mt model in containerised environment for translation,It is pretty hard to work with this models as they need lot of resources to run which significantly increase the cost also,Need of resources,Other model provider API,Docker kubernetes aws resources,Inferenece for LLMs,,,,,,,,,\n",
        "CEO / Co-Founder,1-10,Yes,Writing SQL Queries from Natural Language.,No,Getting it to write a query was relatively easy. Getting it to write a query that would actually be correct AND execute is much harder.,My concerns at this point are primarily focused around getting the LLM to generate an executable query in all cases.,Open AI API,\"Apache Drill, Python.\",Fine tunning LLMS,,,,,,,,,\n",
        "Senior MLOps engineer,\"1,000+\",Yes,Adverse media about suppliers.,\"No, but have plans to do so! I will be involved in the feature store architecture.\",The unpredictable nature of responses.,That they give out results that are way off.,Other model provider API,\"Databricks, Azure\",Embeddings,Not well :D,None yet,,,,,,,\n",
        "machine learning engineer,\"1,000+\",No,,,,,,,,,,,Legal said not to,How do we keep costs down? How do we validate the outputs?,nope,,,\n",
        "CEO,1-10,Yes,Development of production tools for deployment/maintenance of LLMs and other models,We have a platform to develop/run models in production called Statfish,Accuracy and hallucinations,Can't rely on answers 100% of the time,\"Open source model (GPT-J, etc)\",\"Jupyter, kubernetes, haystack\",Inferenece for LLMs,Working on that problem,,,,,,,,\n",
        "ML engineer,50-500,No,,,,,,,,,,,currently using classic ML and I plan to introduce LLM capabilities to my org,\"cost of training and inference, time, gpu, ....\",no,,,\n",
        "Domain Chapter Lead - Data Science and AI,\"1,000+\",Yes,Summarisation of customer calls,nope,View of accuracy and quality of results,Reliability,Open AI API,Azure stack,Embeddings,no framework yet,not much from our side yet,How we manage quality of LLM outputs ?,,,,,,\n",
        "Senior Machine Learning Engineer,\"1,000+\",No,,,,,,,,,,,Lack of applicable business use cases and resources,How do you serve them?,Only for a developer,,,\n",
        "VP of ML,50-500,No,,,,,,,,,,,\"Expensive, no ROI, complex, dangerous\",Is it reliable? How do we know it's reliable?,Yes,Expensive and the business use case was poorly defined. Just a shiny toy,Giggles.,\"Use case examples would be interesting, especially where things have gone wrong.\"\n",
        "Technical Lead,50-500,Yes,\"POC, evaluation for healthtech\",n/a,supplement/support vs replace,user risk,In house model,hf,Fine tunning LLMS,cross fingers,\"not that i can share cheaply, ha\",\"Growing LLM features iteratively & coherently vs Alphabet's \"\"put LLM in all the things!\"\"\",,,,,,\n",
        "Solutions engineer,\"500-1,000\",Yes,Summarizes notes from meetings,no.,Not sure what's the standard,The output can be unpredictable,Open AI API,,Inferenece for LLMs,Human evaluation,,,,,,,,\n",
        "Platform Engineer,50-500,No,,,,,,,,,,,No clear use case for our product,,No,,,How do you decide where to use LLM?\n",
        "Senior MLOps,\"1,000+\",Yes,Chatbot,integrate,\"rate limit,\",\"privacy, trustfulness, latency\",Open AI API,vector databases,Inferenece for LLMs,eval,,,,,,,,\n",
        "Data science manager,\"1,000+\",Yes,classification,no custom tools,scaling :),\"great for batch, terrible for real time\",\"Open source model (GPT-J, etc)\",azure ml suite,Embeddings,human verification of sample of inferences,we fine tune a bert sized model with our data and run the inference in batch. We experimented with quantization and distillation and its pretty impressive how much you can do with much smaller models.,,,,,,,\n",
        "Data Scientist,\"500-1,000\",No,,,,,,,,,,,LLMs and their hallucinations just are not good for our current use cases (healthcare industry),,\"I have not, but I know many of the devs want to; there are some other reasonable use cases too.\",,,How do you get traditional engineers to understand that they shouldn't underestimate the complexity of getting ANYTHING ML-related into prod?\n",
        "Principal Data Scientist,\"1,000+\",Yes,\"Knowledge graph, transcription, topic modeling, embedding everything\",\"Yeah, but it's super secret until we try and sell it\",Slow. They give bad answers when you set the confidence requirements too high,Should we be paying for third party providers or train our own. Alpaca anyone?,Open AI API,\"Nvidia NeMO, Azure OpenAI\",Embeddings,Feedback button or manual review depending on use case,Microsoft's call summarization tool can capture a surprising degree of detail while hilariously completely missing the main point of the meeting,Best practices to prevent leakage for client facing models that employ online learning?,,,,,,\n",
        "Manager Machine learning,\"1,000+\",No,,,,,,,,,,,Less expertise,Large size models latency,,,,\n",
        "Data scientist,1-10,Yes,User facing chat application in a narrow domain.,In progress,,,Open AI API,,Inferenece for LLMs,,,,,,,,,\n",
        "Tech Lead,\"1,000+\",Yes,,,Compliance,\"Compliance, security and legal.\",Open AI API,,Fine tunning LLMS,,,,,,,,,\n",
        "Sr. Director AI/ML,\"1,000+\",Yes,\"ITSM, Sales Prospecting\",chat bot,scalabiltiy,data privacy,\"Open source model (GPT-J, etc)\",kubeflow,Fine tunning LLMS,,,,,,,,,\n",
        "BDM,\"1,000+\",Yes,\"code development, summarisation\",\"development environment, pipelining and deployment tooling\",stack is immature; benchmarks are nonexistent; risk hard/impossible to gauge,evaluating efficacy and ROI; understanding power needs/CO2 emissions,In house model,PyTorch + in-house tools,Inferenece for LLMs,user feedback,\"extremely power-hungry, hard to scale; networking bandwidth is as important as memory when parallelising\",is anyone measuring LLM inference power consumption (and CO2 emissions)? what are the observed ranges of watts/inference of the various model sizes?,,,,,,'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHANGES = ''',What is your position/title at your company?,title,position,other,How big is your organization? (number of employees),Are you using LLM at in your organization?,What is your use case/use cases?,fields,tasks,Have you integrated or built any internal tools to support LLMs in your org? If so what?,integrated,solutions,purposes,What are some of the main challenges you have encountered thus far when building with LLMs,challenges,What are your main concerns with using LLMs in production?,concerns,how are you using LLMs?,What tools are you using with LLMs?,tools,What areas are you most interested in learning more about?,topics,How do you deal with reliability of output?,approaches,Any stories you have that are worth sharing about working with LLMs?,stories,Any questions you have for the community about LLM in production?,questions,What is the main reason for not using LLMs in your org?,reasons,What are some key questions you face when it comes to using LLM in prod?,challenges,have you tried LLMs for different use cases in your org?,tried,cases,\"If yes, why did it not work out?\",problems\n",
        "0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
        "1,Data scientist,Data Scientist,,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Production takes investment,Production Cost,Cost to maintain the service,Cost of Maintenance,Yes,Yes,,Date quality,Data Quality\n",
        "2,ML Engineer,Machine Learning Engineer,,,50-500,Yes,Text summarisation,,Text Summarization,No,No,,,\"Current infrastructure not designed for such massive models, having to implement workarounds for quick fixes\",\"Infrastructure,Workarounds\",Cost,Cost,\"Open source model (GPT-J, etc)\",Seldon,Seldon,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "3,Founder,Founder,,,1-10,Yes,Data annotation; summarization; search,,\"Data Annotation, Text Summarization, Search\",\"https://chat.mantisnlp.com, internal annotation tools\",No,\"MantisNLP, internal annotation tools\",,\"out of date info, hallucinations, cost, difficulty in deploying on own infrastructure\",\"Accuracy,Outdated Information,Cost,Deployment\",they could serve nonsense or out of date information,\"Accuracy, Currency\",Open AI API,,,Working with OSS LLMs,Open Source,we provide links to the context from which answers have been drawn.,Nothing,,,,,,,,,,,,,\n",
        "4,Owner,Owner,,,1-10,Yes,Content production and brainstorming,Content Creation,,Not yet,No,,,not yet,,cost of openai API,Cost,Open AI API,\"Looking at Pinecone, weaviate, langchain, hugging face\",\"Pinecone, Weaviate, LangChain, Hugging Face\",Embeddings,Embeddings,temperature = 0,Temperature Control,,,,,,,,,,,,,\n",
        "5,Senior Principal Scientist,Principal Scientist,Senior,,\"1,000+\",Yes,Healthcare,Healthcare,,\"yes, confidential\",Yes,Confidential,,reliability,Reliability,cost of openai API,Cost,In house model,,,Working with OSS LLMs,Open Source,,,,,,,,,,,,,,,\n",
        "6,Head of Machine Learning,Head of Machine Learning,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,It's only just now becoming feasible with the ChatGPT API. Before it was too expensive and quality was not good enough.,\"Cost, Quality\",How to make them reliable.,Reliability,\"We are still exploring, eg for support in diagnosis of plant diseases.\",No,Plan Diseases,So far it's not reliable enough and we don't have spent enough time preparing special content for it. Like rules it should use during diagnosis. The data ChatGPT was trained on seems a bit hit and miss for us.,\"Reliability, Data Quality, ROI, Business Risks\"\n",
        "7,Senior Machine Learning Engineer,Machine Learning Engineer,Senior,,\"1,000+\",Yes,\"Search, classification, NER\",,\"Search, Text Classification, Named Entity Recognition\",Just a bunch of shitty scripts I need to refactor in the future.,No,Refactoring of scripts,,\"Size in all regards. Also training time, response time and that multi gpu debugging is weird.\",\"Size,Training,Response Time,Debugging\",\"Whether the cost involved for us justifies the gains we get, considering we mainly make money from ads and not a really good product.\",\"Cost, Business Value\",Other model provider API,\"Hugginface, sagemaker, openapi api, tensorflow.\",\"Hugging Face, SageMaker, OpenAPI, TensorFlow\",Working with OSS LLMs,Open Source,\"Ill refer to the picture of the xkcd guy \"\"just stir the model until the numbers look right\"\".\",Manual Tuning,\"I am not sure what ranks as a LLM. Anything bert and beyond? Anyway, we once used setfit for a production task but found that performance was really tricky to debug. The proof of concept was fine. Production was a complete mess. To this day I dont know why- underlying hardware?\",Production issues with LLMs,I mainly wonder how everyone is monitoring these. ALSO. For my company GDPR is a big deal. My DPO shoots down our ideas more often than not or limits it to using the last three months of data. How do other people deal with this?,How do other people monitor LLMs and deal with GDPR limitations?,,,,,,,,,\n",
        "8,Head of Product,,Head,Product,10-50,No,,,,,,,,,,,,,,,,,,,,,,,No applications,Lack of Use Case,ROI of more expensive models,ROI,Yes,Yes,,No ROI,\"Accuracy, Fine-tuning, MLOps Pipeline\"\n",
        "9,,,,,50-500,Yes,,,,oi,No,OI,,lik,,il,,Open AI API,loi,loi,Working with OSS LLMs,Open Source,lo,Nothing,lo,,lo,,,,,,,,,,\n",
        "10,Senior Data Engineer,Data Engineer,Senior,,50-500,Yes,\"text embeddings for search, text generation to help the business people do their job\",Business,\"Text Embeddings, Text Generation\",\"Yes, we're using OpenAI for a couple of internal tools.\",Yes,OpenAI,Internal tools,,,Cost. But our issues with productionisation is more general than just for LLMs,\"Cost, Productionization\",Open AI API,streamlit,Streamlit,Inferenece for LLMs,Inference,\"All internal tools, reliability is not a major concern for now\",Internal Control Systems,,,,,,,,,,,,,\n",
        "11,Machine learning engineer,Machine Learning Engineer,,,\"1,000+\",Yes,\"Natural Language interface, domain specific content creation\",,\"Natural Language Interface, Content Creation\",,,,,Prompt debugging! We need a new form of debugger.,Debugging,They are erratic,Reliability,Open AI API,,,Working with OSS LLMs,Open Source,We have control systems,Control Systems,,,,,,,,,,,,,\n",
        "12,Senior Data Scientist,Data Scientist,Senior,,\"1,000+\",Yes,Chatbots and Text classification,Customer Service,\"Chatbots, Text Classification\",One of our use cases is to build text classification models for each of our customers (we are a B2B company). Achieved this using LLMs and some engineering,Yes,LLMs,Text classification for B2B customers,,,,,\"Open source model (GPT-J, etc)\",,,Embeddings,Embeddings,,,,,,,,,,,,,,,\n",
        "13,ML Engineer,Machine Learning Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
        "14,Snr MLOps Engineer,MLOps Engineer,Senior,,50-500,Yes,Adapters,,Adapters,\"Yes, training setup as well as inference setup\",Yes,LLMs,Training and inference setup,Latency and deployment time,\"Latency,Deployment\",Resources usage along with latency and deployment time,\"Resource Usage, Latency, Deployment\",\"Open source model (GPT-J, etc)\",,,Inferenece for LLMs,Inference,Assessment at evaluation,Model Evaluation,,,,,,,,,,,,,\n",
        "15,,,,,\"1,000+\",Yes,,,,,,,,,,,,,,,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "16,Senior ML engineer,Machine Learning Engineer,Senior,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,\"Two main reasons: 1. Very compute & power hungry 2. Cannot be relied upon as it will very confidently return incorrect information or in ‚Äúexistent facts‚Äù which is problematic for use in my industry.\n",
        "\n",
        "Eg it took us less than 3mn to get BioGPT to tell us vaccines cause autism\",\"Compute, Reliability\",\"What uses cases does it have for our business? how much gain do we get from using an LLM vs a simpler language model where we can control fact and outputs? How much will it cost (to deploy, and run)\",\"Business Use Cases, Cost\",We abandoned the idea pretty quickly,No,,We got terrible suggestions from the LLM that were false information and would be detrimental to the business,\"Domain-specificity, Veracity\"\n",
        "17,Machine Learning Engineer,Machine Learning Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,\"We use medium-sized language models (e.g. RoBERTa-base) as the backbone for our deployed NLP models because they can perform inference on CPU, which is cheap. We don't use LLMs (e.g. ChatGPT, GPT-3) because the APIs are expensive, and fine-tuning them to perform NLP tasks other than language generation (e.g. NER, open-set intent classification) is not straightforward.\",\"Cost, API Limitations, Fine-tuning Difficulty\",How can we ensure the same version of the model is being used? How can we supply a random seed to get deterministic results? How can we minimize LLM API usage costs? How can we fine-tune a LLM on a niche task without having to replicate the weights of the LLM? How can we deploy our own copy of an open-source LLM without breaking the bank?,\"Version Control, Random Seed, API Usage Costs, Fine-tuning, Deployment Cost\",Yes,Yes,,\"We evaluated a LLM (GPT-3) against RoBERTa fine-tuned on our small task-specific dataset, and our fine-tuned model got much higher accuracy than GPT-3 did.\",\"Cost, Use Case\"\n",
        "18,,,,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Not much usecases for us,Lack of Use Case,,,,,,,\n",
        "19,Founding Data Scientist,Data Scientist,Founding,,1-10,Yes,\"Summarization, topic extraction, root-cause analysis\",,\"Text Summarization, Topic Extraction, Root-Cause Analysis\",\"Not much, just some prompt storage and an automated retries to deal with different failure cases.\",No,\"Prompt storage, automated retries\",,\"Hallucinations are a big one. In general, problems are hard to debug without careful manual review, so we're only using them when the amount of results generated is small enough to review by hand.\",\"Debugging,Hallucinations\",\"Reliability, cost, injecting domain-specific language and understanding into them\",\"Reliability, Cost, Domain-specific Language\",Open AI API,,,Embeddings,Embeddings,\"Automated retries for some simple & obvious failures, manual review for everything else\",\"Automated Retries, Manual Review\",,,Where are others drawing the line between training models locally and using LLMs? Have others had success with using LLMs to bootstrap training data for smaller local models?,Where is the line drawn between training models locally and using LLMs? Can LLMs be used to bootstrap training data for smaller models?,,,,,,,,,\n",
        "20,CTO,CTO,,,1-10,Yes,Marketing,Marketing,,Not yet,No,,,Still developing,Development,Cost,Cost,Open AI API,,,Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "21,Machine Learning Engineer,Machine Learning Engineer,,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,Not applicable to the domain,Not Applicable,Not yet,,,,,,\n",
        "22,Head of AI,,Head,AI,\"1,000+\",Yes,Scanning large text corpora using an LLM and some cases with text summary for now,,\"Text Scanning, Text Summarization\",Not (yet).,No,,,It‚Äôs a mix of taming a beast and using alchemy,\"Hardware,Compatibility\",Consistency of the output quality,Output Quality,Other model provider API,./.,,Working with OSS LLMs,Open Source,HITL,Human-in-the-Loop (HITL),They take every Input equally serious which makes for some great laughs‚Ä¶,LLMs provide entertainment value,,,,,,,,,,,\n",
        "23,Product,,,Product,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Production LLM lifecycle and use cases not clear,Production Lifecycle Unclear,Data governance,Data Governance,no,No,,,\n",
        "24,Lead Data Scientist,Data Scientist,Lead,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,Cost / right use-case,\"Cost, Learning Stage\",,,,,,,\n",
        "25,Co-founder,,Co-founder,,1-10,Yes,AI for e-commerce,E-commerce,,No,No,,,Hardware requirements and AI accelerators compatibility,\"Explainability,Expense,Embeddings\",Inference scalability,Scalability,\"Open source model (GPT-J, etc)\",\"PyTorch, AWS SageMaker, OpenAI's API, Midjourney, CLIP/BLIP/etc models\",\"PyTorch, SageMaker, OpenAI, Midjourney, CLIP, BLIP\",Inferenece for LLMs,Inference,,,,,\"Any opinions regarding the newer AI accelerator chips? Many people fighting for GPUs, but not necessarily mentioning AWS Inferentia 2 chips, TPUs, Habana Gaudi, etc.\",\"What are opinions on newer AI accelerator chips like AWS Inferentia 2 chips, TPUs, Habana Gaudi, etc.?\",,,,,,,,,\n",
        "26,MLE,Machine Learning Engineer,,MLE,50-500,Yes,,,,dbt pre-processing to get the text right for the prompts and test it,No,dbt pre-processing,Prompt testing,\"explainability, expense, embeddings\",\"Formatting,Prompt Engineering\",see above,,Open AI API,metaflow,Metaflow,Embeddings,Embeddings,,,,,,,,,,,,,,,\n",
        "27,Snr MLE,Machine Learning Engineer,Senior,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,Exploration stage,\"Cost, Latency, Accuracy\",Putting company data in the right structure and updating the model with it,Data Preparation,yes,Yes,,getting fine tune right and LLMops pipeline,Model Size\n",
        "28,Director of Artificial Intelligence,,Director,Artificial Intelligence,\"1,000+\",Yes,\"Novelty - ChatGPT is being explored to optimize code, critique written works, etc.\",,\"Code Optimization, Written Work Critique\",No,No,,,,,Complacency and atrophy of critical thinking,Critical Thinking,\"Open source model (GPT-J, etc)\",Microsoft Azure OpenAI services,\"Microsoft Azure, OpenAI\",Fine tunning LLMS,Fine-Tuning,Educate others on the limitations and proper use.,Education on Limitations and Proper Use,\"One of our citizen data scientists now (figuratively) \"\"worships\"\" ChatGPT Gods given its capabilities. LOL.\",Citizen data scientist impressed with ChatGPT capabilities,None at the moment given our priorities are mostly with computer vision and time-series data analysis.,Any questions or opinions regarding LLMs?,,,,,,,,,\n",
        "29,Data Scientist,Data Scientist,,,50-500,Yes,\"Summarisation, Email generation, Information retrieval\",,\"Text Summarization, Email Generation, Information Retrieval\",Modular python library for interacting with multiple LLM providers/APIs and a standardized prompt generation framework.,No,Modular python library,\"Interacting with multiple LLM providers/APIs, standardized prompt generation framework\",Consistency of formatting in freeform LLM text generation. Subtleties in prompt engineering.,Reproducibility,\"Detecting truthfulness, hallucination. Output validation. Risk of monopolisation and centralised access to LLM as a service\",\"Truthfulness, Validation, Monopolization\",Other model provider API,Python,Python,Working with OSS LLMs,Open Source,,,,,,,,,,,,,,,\n",
        "30,Co-founder and CTO,,Co-founder and CTO,,1-10,Yes,Prompt autogeneration and synthetic data generation,,\"Prompt Autogeneration, Synthetic Data Generation\",,,,,Reproducibility,Latency,Hallucination and latency,\"Hallucination, Latency\",Open AI API,HoneyHive,HoneyHive,Fine tunning LLMS,Fine-Tuning,\"Multi-responses shown to user side by side, ghost writer style dropout\",\"Multi-Responses, Dropout\",,,How are you thinking about tracking user feedback in production?,How to track user feedback in production?,,,,,,,,,\n",
        "31,,,,,50-500,Yes,,,,,,,,,,,,,,,Working with OSS LLMs,Open Source,,,,,,,,,,,,,,,\n",
        "32,Senior Software Enginner,Software Engineer,Senior,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,No business need yet,Not in Domain,,,,,,,\n",
        "33,Co-founder,,Co-founder,,1-10,No,,,,,,,,,,,,,,,,,,,,,,,We're learning,\"Dialog Management, Answer Relevance, Practical Jokes, Failover\",,,,,,,\n",
        "34,ML Developer,Machine Learning Developer,,,\"500-1,000\",Yes,Question Answering,,Question Answering,yes,Yes,BERT,Embeddings,latency,Latency,\"latency, factuality\",\"Latency, Factuality\",\"Open source model (GPT-J, etc)\",,,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "35,,,,,1-10,No,,,,,,,,,,,,,,,,,,,,,,,\"Cost, Latency, Accuracy\",No Business Need,\"Cost can be high, hallucinations\",\"Inference Cost, Hallucinations\",,,,,\n",
        "36,data scientist,Data Scientist,,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,not really in our domain,Startup Use Planned,,,no - closest we've got to dealing with language is fuzzy lookups,No,Fuzzy Lookups,,\n",
        "37,Senior Machine Learning Engineer,Machine Learning Engineer,Senior,,10-50,Yes,Question generation for accountants,Accounting,Question Generation,built internal,No,,,reducing latency,Infrastructure Costs,not knowing the data used for the initial training,Data Transparency,In house model,\"TensorFlow, TFX\",\"TensorFlow, TFX\",Inferenece for LLMs,Inference,yes,Nothing,,,,,,,,,,,,,\n",
        "38,Founder,,Founder,,1-10,Yes,Zero-shot/Few Shot classification for data enrichment,,\"Zero-shot/Few Shot Classification, Data Enrichment\",\"Internally, we use BERT to create embeddings, then train a smaller model on customer data.\",,,,Infrastructure costs.,Result Analysis,\"Is what I;m building getting obsolete in 3 months,\",Obsolescence,In house model,In house Bert (from Huggingface) + Open AI,\"In house Bert, OpenAI\",Fine tunning LLMS,Fine-Tuning,I am stuck with this right now - can I integrate these into the UI for AI Hero.,UI Integration,ChatGPT API output is different every time for the same input - makes it hard to create a reliable service.,Reliability with ChatGPT API is hard,How are you managing training/deployment of these?,How to manage training/deployment of LLMs?,,,,,,,,,\n",
        "39,SRE,,SRE,,\"1,000+\",Yes,analyze logs,,Log Analysis,no,,,,Analyzing results,\"Automation,Scaling,Transfer Learning,Tuning,Reinforcement Learning\",security,Security,In house model,in house built tool,In house built tool,Fine tunning LLMS,Fine-Tuning,Very manual so far.,Manual Review,confidential,,no,,,,,,,,,,\n",
        "40,\"Cofounder (normally product manager, but also wearer of many hats)\",,Co-founder,Product Manager,1-10,No,,,,,,,,,,,,,,,,,,,,,,,\"1. gcm diagflow is good for complete basic dialogs, 2. halucinations 3.a. how to make answers be focused on what my company is doing rather than general answers, 3.b. how to keep that evolving as we learn more about the customers 3.c. how to prevent a practical joker who's conversing with my llm to inject some practical joke into it 4. if im relying on openai-s llm, i need to always get a reply what happens if they're busy, whats the proverbial 'failover'\",Data Privacy,how to get it to use my company's specific knowledge,Domain Specific Knowledge,not thoroughly enough yet,No,,still working on it,\"Domain-specificity, Veracity\"\n",
        "41,Data Architect,Data Architect,,,\"1,000+\",Yes,Search and chat on proprietary data,,\"Search, Chatbot\",No,No,,,\"Automation amd scaling of transfer learning, tuning and reinforcement learning\",,Scaling,Scaling,Open AI API,\"Lang chain, k8s, gcp\",\"LangChain, Kubernetes, GCP\",Learning from new/private data,Learning from New/Private Data,Not a concern. In-house,Nothing,No,,\"Best automated/scalable pipeline for tuning,, transfer learning (augmentation), reinforcement feedback\",\"What is the best pipeline architecture for LLM tuning, transfer learning, and reinforcement feedback?\",,,,,,,,,\n",
        "42,Senior software engineer,Software Engineer,Senior,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,No need atm,\"Infrastructure Evaluation, Traceability\",Reliability,Reliability,,,,,\n",
        "43,consultant,,Consultant,,1-10,No,,,,,,,,,,,,,,,,,,,,,,,we'll be using at a startup I'm part of,Lack of Use Case,cost and fit for purpose. Actual market validation,\"Cost, Market Validation\",yes. I've used them for classification,Yes,Classification,they are in prod for that use case,None\n",
        "44,CTO,,CTO,,50-500,Yes,\"Product (Software) recommendations, Internal Q&A, Marketing content, and Customer support\",\"E-commerce, Customer Support, Marketing\",\"Recommendation, Question Answering, Text Generation\",\"Langchain, OpenAI Playground, ChatGPT, PromptLayer, Weaviate\",No,\"Langchain, OpenAI Playground, ChatGPT, PromptLayer, Weaviate\",,Rapidly changing APIs,API changes,Hallucinations misleading customers,\"Hallucinations,Misleading customers\",Open AI API,PromptLayer,PromptLayer,Embeddings,Embeddings,\"Pre/post-processing, constitutional / self-auditing, customer feedback thumbs up/down, custom embedding matrix\",\"Pre/Post-Processing, Self-Auditing, Customer Feedback, Custom Embedding Matrix\",,,How might you avoid siloing all of your embeddings when different teams need different tools without building it all in house yet all would benefit from the combined collection?,How to avoid siloing embeddings when different teams need different tools without building it all in house?,,,,,,,,,\n",
        "45,Data scientist / co-founder,Data Scientist,,Co-founder,1-10,Yes,Information retrieval and text generation.,,\"Information Retrieval, Text Generation\",\"We have some output validation methods and templates, basically regex matching prompts.\",No,,Output validation,Input length limitation and having to work around it with non-optimal methods.,Input length limitation,Hallucinations,Hallucinations,Open AI API,\"Langchain, Huggingface, Polars\",\"LangChain, HuggingFace, Polars\",Fine tunning LLMS,\"Fine-Tuning, Embeddings\",We implement checking with less stochastic tools.,Stochastic Tools Checking,Content language matters a lot!,Content language is important for LLMs,,,,,,,,,,,\n",
        "46,Analytics engineer,Analytics Engineer,,,50-500,Yes,Chatbot,,Chatbot,In POC at the moment,No,,POC,Vectorizing CRUD and ci / CD,\"CI/CD,Performance\",To get better at semantic search,Semantic search,Open AI API,\"Langchain, pinecone, slack and aws\",\"LangChain, Pinecone, Slack, AWS\",Embeddings,Embeddings,,,,,\"How to better deploy python apps using aws and operationalizing it.\n",
        "How to better manage vector indexes.\",How to deploy python apps using AWS and manage vector indexes?,,,,,,,,,\n",
        "47,\"Director, Innovation\",,Director,Innovation,50-500,No,,,,,,,,,,,,,,,,,,,,,,,Data privacy concerns,,Difficulty or ease of maintenance; costs; data privacy,\"Maintenance, Costs, Data Privacy\",Not yet,No,,na,None\n",
        "48,ML & MLOps practice lead,ML & MLOps Practice Lead,,,50-500,Yes,\"Text classification, text generation\",,\"Text Classification, Text Generation\",,,,,\"Compute performance, fine tuning challenges, explainability\",\"Performance,Explainability,Fine-tuning\",,,\"Open source model (GPT-J, etc)\",,,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "49,Founder,Founder,,,1-10,Yes,Social media idea generation,Social Media,Text Generation,No just used langchain and OpenAI,No,\"Langchain, OpenAI\",,Prompt engineering; handling very large documents,\"Prompt engineering,Handling large documents\",Bad data/completions; runtime errors parsing completions (we use regex to parse each line ),\"Bad data,Runtime errors\",Open AI API,LangChain,LangChain,Fine tunning LLMS,Fine-Tuning,We don‚Äôt do anything right now,Nothing,,,,,,,,,,,,,\n",
        "50,CTO,CTO,,,1-10,Yes,Semantic data extraction and corporate governance,Corporate Governance,Semantic Data Extraction,\"I'm unable to utilise LLM's currently due to data security restrictions, however as a OpenAI beta tester, around 2 years ago I created a business Operational Delivery Framework for a ¬£500M nuclear project. I then transferred the learning framework to ¬£5B project.\",No,,Operational Delivery Framework,Environment dependency conflicts,Environment dependency conflicts,Safe and ethical usage,\"Safety,Ethics\",Open AI API,Use case dependent,,Fine tunning LLMS,Fine-Tuning,\"AI is replaced by HI(Human) for final check, always.\",Human-in-the-Loop (HITL),\"Yes, the ODF I created was regarded as best in class by regulators and has been suggested to be implemented as new gold standard. I used GPT3 to advise on the design of the framework, and to write all the necessary code to create it.\",GPT3 used to create best in class ODF framework,\"Nope, no questions, just a great big thankyou, and a virtual high five for all the awesome peeps üôè\",,,,,,,,,,\n",
        "51,MLOps Engineer,MLOps Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,Still evaluating infrastructure requirements and ways to make it traceable if things go wrong,,\"Retraining, tackle hallucinations and avoid wrong results\",\"Retraining, Hallucinations, Accuracy\",No,No,,,\n",
        "52,Junior developer,,Junior,Developer,1-10,Yes,Creating a chat bot to query data efficiently for managers,,\"Chatbot, Information Retrieval\",Not yet but its in production. Using langchain.,Yes,Langchain,Production,\"The main challenges are using the different tools from langchain to optimise for the use case. There are a lot of things to consider. Also, we use ts and angular front end so not having all the tools that are available in python required us to have python as the backend now.\",\"Optimization,Interoperability\",It‚Äôs how useful/valuable it will actually be for the client.,Value for client,Open AI API,Langchain/ts/csharpe/angular.,\"LangChain, TypeScript, C#, Angular\",Fine tunning LLMS,Fine-Tuning,We havent gotten to that yet.,Nothing,\"They are amazing. But as mentioned, a chatbot alone is probably not enough to be useful, hence building out other capabilities will become important.\",Integrations are important for chatbot,,,,,,,,,,,\n",
        "53,deep learning engineer,Deep Learning Engineer,,,\"1,000+\",Yes,extract information from image and text of medical domain,Medical,\"Information Extraction, Image Analysis, Text Classification\",no,No,,VSCode Extension,deploy models into production,Deployment,hight training & inference cost,\"Training cost,Inference cost\",\"Open source model (GPT-J, etc)\",\"transformers, triton inference server\",\"Transformers, Triton Inference Server\",Working with OSS LLMs,Open Source,,,,,,,,,,,,,,,\n",
        "54,Co-Founder,Co-Founder,,,10-50,No,,,,,,,,,,,,,,,,,,,,,,,Lack of use cases that require so much sophistication,,\"cost of maintenance, lack of institutional knowledge\",\"Cost of Maintenance, Institutional Knowledge\",No,No,,,\n",
        "55,Intern,,Intern,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Because I am in education sector.,Education sector,when you use LLM in production it can fail any time,Reliability,NO,NO,,,\n",
        "56,CTO,CTO,,,1-10,Yes,Coding,,Coding,VSCode Extension,No,,Open source tools,Work with 3 AI providers at the same time,Interoperability,The wrong respond from the model,Incorrect responses,Open AI API,Code GPT https://codegpt.co,Code GPT,Embeddings,Embeddings,Now it is just responds tickets in github,GitHub Tickets,\"The extension hace more than 200,000 downloads and it work great!\",LLM leads to great success,,,,,,,,,,,\n",
        "57,Software Development Engineer,Software Development Engineer,,,\"1,000+\",Yes,\"(Not in my org, at work I don‚Äôt do this, it‚Äôs personal) Cover letter generator based on resume and job description\",,Text Generation,,,,,Embedding with appropriate data effectively,Data embedding,Being dependent on one company for the API so far. Having models of equivalent performance whose weights were public would be a great relief,Dependency on one company,Open AI API,\"Gpt index, langchain\",\"GPT Index, LangChain\",Embeddings,Embeddings,\"Prompt engineering, allow user to be specific, and of course re-generate\",\"Prompt Engineering, User Feedback, Regeneration\",,,,,,,,,,,,,\n",
        "58,Data Science Team Lead,Data Science Team Lead,,,10-50,Yes,Topic sentiment evaluation on text,,\"Sentiment Analysis, Topic Modeling\",,,,,Managing training data labeling,Data labeling,explainability,Explainability,\"Open source model (GPT-J, etc)\",AWS Sagemaker,AWS Sagemaker,Fine tunning LLMS,Fine-Tuning,Test each new model candidate on test set,Model Evaluation,,,,,,,,,,,,,\n",
        "59,Sr. ML Eng,Sr. ML Eng,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,inference cost vs. impact on revenue,Return of Investment,how to reduce inference cost,Inference Cost,,,,,\n",
        "60,Senior AI Expert,Senior AI Expert,,,50-500,Yes,Medical Diagnostics Support,Medical,Diagnostics Support,\"No, we rely on open source tools\",No,Langchain,Bot serving,\"Explainability, Testing, Getting ground truth data\",\"Explainability,Testing,Data labeling\",Trustworthiness,Trustworthiness,\"Open source model (GPT-J, etc)\",\"Milvus DB, FastAPI, Hugginface, Pytorch, etc.\",\"Milvus DB, FastAPI, Hugging Face, PyTorch\",Working with OSS LLMs,Open Source,We try to make the output as transparent as we can (using explainability techniques) and not relying upon LLM for the end decision (these are used as a support tool),\"Model Explainability, Human Decision Making\",,,,,,,,,,,,,\n",
        "61,Data Science Engineer,Data Science Engineer,,,50-500,Yes,Assistant chat bot,,Chatbot,\"Langchain. Will serve the bot from a flask server, being set up for the bot, into our existing java platform\",Yes,\"GPT3.5, PyTorch, AWS Lambda, Docker (ECR/ECS)\",\"Numerical data processing, Commentary generation, Visual report generation\",\"Langchain LLM Parse error. Turbo not using multiple tools for some reason, when davinci-003 did\",Parse error,\"Need HIPAA compliance, so trying to get on Azure. Worried about the contract falling through\",\"HIPAA compliance,Contract falling through\",Open AI API,Langchain. All my agent‚Äôs tools are custom-built. I hit our internal APIs with them,LangChain,I want deep knowledge on how to make the ReAct pattern work in various edge cases,\"ReAct Pattern, Edge Cases\",\"Will have friendly internal users using it first in the course of serving customers, and giving us per-message feedback\",User Feedback,davinci-003 hallucinating using an existing tool without actually using it!,LLM hallucination without actual usage,How should I split my initial prompt up between turbo‚Äôs System prompt and the first Human prompt I give it?,How to split initial prompt between turbo's System prompt and the first Human prompt?,,,,,,,,,\n",
        "62,Head of Data and ML,,Head,Data and ML,10-50,No,,,,,,,,,,,,,,,,,,,,,,,\"No Use case, being a fintech startup, besides helping with processing documents\",\"Lack of Use Case, except for processing documents\",why waste the money,Cost,no,no,,,\n",
        "63,ML/MLOps Engineer,ML/MLOps Engineer,,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,No clear use case,Lack of Use Case,\"Running them, fine tuning, cost\",\"Running, Fine-tuning, Cost\",Nope,Nope,,,\n",
        "64,\"Data Scientist, HappyFresh\",Data Scientist,,HappyFresh,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,Not,,,,,,,,\n",
        "65,Data Scientist,Data Scientist,,,10-50,Yes,Automation of report writing.,,Report Generation,\"Yes.\n",
        "Responsible for the design, building, testing and deployment of a GPT-3.5 application that processes numerical data and free-response text in tabular form to generates commentary and analysis for a visual report. Researchers are only required to upload a zip folder(1-click).\n",
        "Technologies: AWS lambda, Docker (ECR/ECS), GPT3.5, PyTorch.\",No,HF Inference Endpoints,,\"control of the output for our usecase. Reports need to understand the client's intent and take information from briefing calls and key erquriements. These can help guide the output, but now always.\",Output control,Thta it will hallucinate something that we won;t pick up in the report editing phase,Hallucinations,Open AI API,Torch and Pandas,\"Torch, Pandas\",Prompt Engineering (Riley Goodside),Prompt Engineering,Have editors comb through it,Manual Review,We aim to turn a report around from survey finishing to client inbox in 24 hour. So far we are at 48 hours.,Challenges with production environment for vector lookup system,,,,,,,,,,,\n",
        "66,Data Scientist,Data Scientist,,,50-500,Yes,\"LLM in Marketing, Custom data\",\"Marketing, Custom Data\",Text Generation,HF Inference Endpoints,Yes,\"RASA, Haystack\",\"Chatbot, Question Answering\",Cloud Cost,Cloud cost,No proper documentation,Documentation,Other model provider API,,,Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "67,Senior Data Scientist,Data Scientist,Senior,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,\"We don't have a need for generating unstructured data, we work in small languages, our text is very domain specific, LLMs are very compute intensive -> expensive to run and take a lot of work to set up.\",\"Domain specific language, compute intensive, expensive to run and set up\",\"Cost, latency, actual performance in the languages / domains we work with (real languages, not programming)\",\"Cost, Latency, Performance in Real Languages\",Nope. Tried some BERT for classification but that hardly counts as an LLM. And I've made models as big as BERT I think.,Nope,Classification,,\n",
        "68,Data Scientist,Data Scientist,,,50-500,Yes,chatbot/voice assistant,,\"Chatbot, Voice Assistant\",just used RASA for chatbot creation and haystack for open domain question answering,Yes,,High scale server on K8's,\"currently used only pre-trained models, biggest challenge was to obtain data for specific domain fine tuning\",Data acquisition,it is very important in my opinion to accurately monitor the kind of usage to prevent frauds or malicious intentions,\"Usage monitoring,Fraud prevention\",Other model provider API,Haystack,Haystack,Fine tunning LLMS,Fine-Tuning,\"it depends from specific context and specific use cases, in my experience I rely on validation set and a validation tool that estimates accuracy of answers based on answers manually scrutinized by humans\",\"Validation Set, Validation Tool, Human Scrutiny\",not so far,,it would be interesting a discussion about how to control the output and in general the kind of usage the system has been exposed,How to control the output and usage of LLMs?,,,,,,,,,\n",
        "69,Head of Data Science,,Head,Data Science,\"1,000+\",Yes,\"Entity matching, customer service responses (souped up/targetted FAQ).\",Customer Service,\"Entity Matching, Text Classification\",yes - high scale server on K8's,No,,Additional tools,K8's caches... TESTING.,Testing,\"cost, latency, truthfulnes - but also\n",
        "\n",
        "Maybe management in the sense of versioning and resolving / attributing issues?\n",
        "\n",
        "Copyright and intellectual property liability - who knows what it's been trained on and what it might spit out... What happens if it provides a Disney script for an advert? (I know the answer - nothing good).\n",
        "\n",
        "Safety : what if it tells someone to kill themselves? Or spits out a recipe to make smallpox? (I am being kind of silly here, but... not entirely)\",\"Cost,Latency,Truthfulness,Versioning,Issue resolution,Copyright\",\"Open source model (GPT-J, etc)\",\"not sure what this means - we use k8's and cloud functions to run them, also interface to case management solutions.\",,calibration of llm output / calbration and testing of llms,\"Calibration, Testing\",badly. We do lots of testing but I constantly feel we are on shaky ground.,Extensive Testing,\"We had a nice time selecting models, proving it would all work and using TSNE and PCA to reduce the dimensionality of embeddings and get compression for our vector lookup system... Very good and satisfying work, we got prizes and praising from all. Then camesome dreadful times making our vector lookup system work under load for long durations in a proper prod environment (where we couldn't touch it to do things like restart it or reload the index). I think this will be a big issue in the future - we need things that are as reliable as oracle for running the inferences, we do not have them - everything is glued together (I am guessing this is not true if you work for Google...)\",No clear advantage of LLMs over simpler models,,,,,,,,,,,\n",
        "70,Founder,,Founder,,1-10,Yes,We are making tools to monitor and improve the performance of LLMs,,\"Performance Monitoring, Model Improvement\",No additional tools,No,,Database connections,,,Hallucinations,\"Safety,Hallucinations,Intellectual property liability\",\"Open source model (GPT-J, etc)\",UpTrain,UpTrain,Embeddings,Embeddings,We are building a tool for that,Tool Development,,,How is everyone thinking about fine-tuning LLMs for their use-cases?,How to fine-tune LLMs for specific use-cases?,,,,,,,,,\n",
        "71,CEO,CEO,,,1-10,Yes,\"Embeddings, Feature Extraction, Chatbot\",,\"Embeddings, Feature Extraction, Chatbot\",Database connections,Yes,Open source LLM,Deployment,Context length limitations,Context length limitation,\"Hallucinations, rate limiting,\",,Open AI API,\"Langchain, Llama Index, Deep lake, Weaviate\",\"LangChain, Llama Index, Deep Lake, Weaviate\",Fine tunning LLMS,\"Fine-Tuning, Embeddings\",Error handling,Error Handling,,,,,,,,,,,,,\n",
        "72,Founder,,Founder,,1-10,Yes,\"Text summarization, code generation\",,\"Text Summarization, Code Generation\",No,Yes,Model store,,Dealing with vector stores,Vector stores,Keeping agent on track with system (OpenAI‚Äôs ChatGPT) or role,\"Keeping agent on track,Role\",Open AI API,LangChain,LangChain,Embeddings,Embeddings,,,,,What are some of the best practices for dealing with LLMs or foundation models?,What are the best practices for dealing with LLMs or foundation models?,,,,,,,,,\n",
        "73,Senior Dara scientist,Senior Data Scientist,,,\"1,000+\",Yes,\"Chatbot, semantic search\",,\"Chatbot, Semantic Search\",Deploy open source LLM,,,,\"Data privacy, big models\",\"Data privacy,Model size\",Fake information,Fake information,\"Open source model (GPT-J, etc)\",PyTorch huggingaface,\"PyTorch, Hugging Face\",Inferenece for LLMs,Inference,Generate multiple output,Multiple Output Generation,Not really,,Chatgpt for domain knowledge,How to use Chatgpt for domain knowledge?,,,,,,,,,\n",
        "74,ML Engineer,ML Engineer,,,\"1,000+\",Yes,Customer support,Customer Support,,We‚Äôve integrated with model store!,,,,Memory requirements during inference,\"Memory,Inference,Python\",Slow start up times/it‚Äôs very different from regular code,\"Slow start up times,Different from regular code\",In house model,Hugging Face,Hugging Face,Inferenece for LLMs,Inference,We‚Äôre doing classification not generation,\"Model Evaluation, Task-Specific Metrics Tracking\",,,Curious to hear about other use cases!,What are other use cases for LLMs?,,,,,,,,,\n",
        "75,Director,,Director,,1-10,No,,,,,,,,,,,,,,,,,,,,,,,No need yet,No need yet,I don't need them,,\"Yes, too heavy\",Yes,Too Heavy,Too heavy,Model Size\n",
        "76,Technical Marketing Manager,Technical Marketing Manager,,,10-50,Yes,Content marketing and text drafting,Content Marketing,Text Drafting,not at the moment in my department,No,,,,,costs under control and reliability,\"Costs,Reliability\",\"Open source model (GPT-J, etc)\",,,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "77,Head of MLOps,,Head,MLOps,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,\"Currently assessing the risk and potential applications, performing POCs.\",\"Assessing risk and potential applications, performing POCs\",\"What are the potential risks, where are the benefits worth the costs for enterprise grade integration, how do we make these things secure, how do we monitor?\",\"Security, Monitoring, ROI\",In the process of doing so.,In the process,,We‚Äôre being thorough.,Thoroughness\n",
        "78,CDO,,CDO,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Wee are exploring the space but don't feel they are ready for production use yeet,Not ready for production use yet,\"How to pin them to a domain, how to verify the veracity of teh outcome and how to best chain them with APIs\",\"Domain Specific Knowledge, Outcome Verification, API Integration\",We are exploring customer facing and colleague facing applications,We are exploring,Customer and Colleague Facing Applications,Concerns about sticking to a domain and about veracity if the answers,\"Domain-specificity, Veracity\"\n",
        "79,Owner,,Owner,,1-10,Yes,\"sales pipelines, text generation, data classification and many more\",Sales,\"Text Generation, Data Classification\",\"Yes, several\",Yes,chatgpt api,Code generation,The need to use Python,Computing Power,\"Cost, accuracy\",\"Costs,Accuracy\",Open AI API,Zapier,Zapier,Finetuning and embeddings,\"Fine-Tuning, Embeddings\",extensive testing,,,,,,,,,,,,,,\n",
        "80,Agricultue consultant,,Agriculture Consultant,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,$,Cost,$,Cost,No,No,,No,None\n",
        "81,Data scientist,Data Scientist,,,1-10,Yes,,,,Yes,Yes,opus-mt model,Translation,Computing power,\"Sparse Representations,Aggregation\",,,Open AI API,,,Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "82,Product Owner Data Science,Product Owner Data Science,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,MLOps and DataScience rather young discipline. We are currently working on an LLM use case though,\"MLOps and DataScience rather young discipline, currently working on an LLM use case\",\"Runtime, Drift detection\",\"Runtime, Drift Detection\",No,No,,,\n",
        "83,Team Lead DS & MLE,,Team Lead,DS & MLE,50-500,No,,,,,,,,,,,,,,,,,,,,,,,Domain specific challenges for what we do. Digging into how we could train LLM to fit the needs.,\"Domain specific challenges, exploring how to train LLM to fit the needs\",What is it? There is just the huge gap in understanding about analyzing text vs generating text.,\"Understanding, Analyzing vs Generating Text\",We were using s-BERT in our primary recsys model. Recent modeling efforts have reset to simpler systems that are performing better. Still using it in a 2nd model for a clustering challenge.,Yes,\"Recommendation System, Clustering\",,\n",
        "84,Co-founder,,Co-founder,,1-10,Yes,Semantic vector embeddings as a feature into RecSys,Recommender Systems,Semantic Vector Embeddings,Not yet,No,,,\"Sparse representations (e.g. tfidf) are easier to work with for our use-cases, because they survive simple aggregation better.\",,Explainability of embeddings.,Explainability of embeddings,\"Open source model (GPT-J, etc)\",,,Embeddings,Embeddings,We have come up with a large list of proxy metrics that we found correlate with performance on our tasks - so we track those.,\"User Verification, Spreadsheet Interface\",\"As far as embeddings are concerned, we haven't found a clear advantage of LLMs vs simpler models (tfidf) trained on a relevant dataset. Both require elaborate preprocessing of the embedded documents for example.\",,I'd love to connect with others who use LLMs embeddings and talk in more detail :),Looking to connect with others who use LLMs embeddings.,,,,,,,,,\n",
        "85,Machine Learning Engineer,Machine Learning Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,it isn't in focus on my leaders to use LLMs right now,Not in focus of leaders to use LLMs right now,Two key questions I do myself on LLM in prod are: 1 - How is the best strategy to train or fine-tune a LLM ? 2 - How to manage infrastructure costs ?,\"Training Strategy, Infrastructure Costs\",not yet,No,,.,None\n",
        "86,CEO,CEO,,,1-10,No,,,,,,,,,,,,,,,,,,,,,,,\"No current need, but also tooling\",\"No current need, lack of tooling\",\"Reliability, safety, certainty, hallucinations, etc\",\"Reliability, Safety, Certainty, Hallucinations\",No,No,,,\n",
        "87,Principal Cloud Developer,Cloud Developer,Principal,,50-500,Yes,\"text resume, make ppts, text quizzes, text to flash cards, video resume etc,etc\",,\"Text Resume, PowerPoint Generation, Text Quizzes, Text to Flash Cards, Video Resume\",-,,,,-,\"Enterprise Adoption,Uncertainty\",\"bad output, costs etc,etc\",\"Bad output,Costs\",Open AI API,-,,Fine tunning LLMS,Fine-Tuning,-,,-,No experience with LLMs yet,-,,,,,,,,,,\n",
        "88,CPO,CPO,,,1-10,Yes,We help organizations transition Excel reports to Python,,Excel Report Transition to Python,\"Yes -- our product, Mito, is a spreadsheet that generates Python code as you edit it. We have integrated the chatgpt api into Mito so users can generate code using the LLM. Then, they can use the Mito spreadsheet to verify that the code it generated is correct.\",Yes,chatgpt api,Code verification,We are not yet sure how large enterprises are going to adopt LLMs. Our large enterprise customers are currently not using the Mito AI feature because there is too much uncertainty.,\"Resource Requirements,Cost\",,,Open AI API,Just the Mito spreadsheet,Mito Spreadsheet,Figuring out how to deploy LLMs for enterprise use,Deployment,We give users a spreadsheet interface to verify the output of the LLM. Spreadsheets are great for understanding changes to data!,Working on the Problem,,,,,,,,,,,,,\n",
        "89,Cloud MLOps Engineer,MLOps Engineer,,Cloud,10-50,Yes,Translate french text to english,,Translation,Yes we have used opus-mt model in containerised environment for translation,Yes,opus-mt model,Translation,It is pretty hard to work with this models as they need lot of resources to run which significantly increase the cost also,\"Query Writing,Accuracy,Execution\",Need of resources,Need of resources,Other model provider API,Docker kubernetes aws resources,\"Docker, Kubernetes, AWS Resources\",Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "90,CEO / Co-Founder,CEO / Co-Founder,,,1-10,Yes,Writing SQL Queries from Natural Language.,,SQL Query Generation from Natural Language,No,No,,,Getting it to write a query was relatively easy. Getting it to write a query that would actually be correct AND execute is much harder.,Unpredictability,My concerns at this point are primarily focused around getting the LLM to generate an executable query in all cases.,Generating executable query,Open AI API,\"Apache Drill, Python.\",\"Apache Drill, Python\",Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "91,Senior MLOps engineer,MLOps Engineer,Senior,,\"1,000+\",Yes,Adverse media about suppliers.,Risk Management,Adverse Media Analysis,\"No, but have plans to do so! I will be involved in the feature store architecture.\",No,,Feature store architecture,The unpredictable nature of responses.,\"Accuracy,Hallucinations,Quality\",That they give out results that are way off.,Inaccurate results,Other model provider API,\"Databricks, Azure\",\"Databricks, Azure\",Embeddings,Embeddings,Not well :D,,None yet,,,,,,,,,,,,\n",
        "92,machine learning engineer,Machine Learning Engineer,,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Legal said not to,Legal restrictions,How do we keep costs down? How do we validate the outputs?,\"Cost, Output Validation\",nope,No,,,\n",
        "93,CEO,CEO,,,1-10,Yes,Development of production tools for deployment/maintenance of LLMs and other models,Model Development,Production Tool Development for LLMs and Other Models,We have a platform to develop/run models in production called Statfish,,Statfish,Model development and production,Accuracy and hallucinations,\"Accuracy,Quality\",Can't rely on answers 100% of the time,Unreliable answers,\"Open source model (GPT-J, etc)\",\"Jupyter, kubernetes, haystack\",\"Jupyter, Kubernetes, Haystack\",Inferenece for LLMs,Inference,Working on that problem,,,,,,,,,,,,,,\n",
        "94,ML engineer,Machine Learning Engineer,ML,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,currently using classic ML and I plan to introduce LLM capabilities to my org,Plan to introduce LLM capabilities to the organization,\"cost of training and inference, time, gpu, ....\",\"Cost, Training, Inference, GPU\",no,No,,,\n",
        "95,Domain Chapter Lead - Data Science and AI,Domain Chapter Lead,,Data Science and AI,\"1,000+\",Yes,Summarisation of customer calls,Customer Support,Call Summarization,nope,No,,,View of accuracy and quality of results,\"Supplement,Support,Replacement\",Reliability,Reliability,Open AI API,Azure stack,Azure Stack,Embeddings,Embeddings,no framework yet,,not much from our side yet,,How we manage quality of LLM outputs ?,How to manage quality of LLM outputs?,,,,,,,,,\n",
        "96,Senior Machine Learning Engineer,Machine Learning Engineer,Senior,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Lack of applicable business use cases and resources,Lack of applicable business use cases and resources,How do you serve them?,Model Serving,Only for a developer,No,Development,,\n",
        "97,VP of ML,VP of ML,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,\"Expensive, no ROI, complex, dangerous\",\"Expensive, no ROI, complex, dangerous\",Is it reliable? How do we know it's reliable?,Reliability,Yes,Yes,,Expensive and the business use case was poorly defined. Just a shiny toy,\n",
        "98,Technical Lead,Technical Lead,,,50-500,Yes,\"POC, evaluation for healthtech\",Healthcare,\"POC, Evaluation\",,,,,supplement/support vs replace,Standardization,user risk,User risk,In house model,hf,Hugging Face,Fine tunning LLMS,Fine-Tuning,cross fingers,Hope,\"not that i can share cheaply, ha\",,\"Growing LLM features iteratively & coherently vs Alphabet's \"\"put LLM in all the things!\"\"\",How to grow LLM features iteratively and coherently?,,,,,,,,,\n",
        "99,Solutions engineer,Solutions Engineer,,,\"500-1,000\",Yes,Summarizes notes from meetings,,Meeting Note Summarization,no.,No,,,Not sure what's the standard,Rate Limiting,The output can be unpredictable,Unpredictable output,Open AI API,,,Inferenece for LLMs,Inference,Human evaluation,Human Evaluation,,,,,,,,,,,,,\n",
        "100,Platform Engineer,Platform Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,No clear use case for our product,Lack of Use Case,,,No,No,,,\n",
        "101,Senior MLOps,MLOps,Senior,,\"1,000+\",Yes,Chatbot,,Chatbot,integrate,,,Integration,\"rate limit,\",Scalability,\"privacy, trustfulness, latency\",\"Privacy,Trustfulness,Latency\",Open AI API,vector databases,Vector Databases,Inferenece for LLMs,Inference,eval,Model Evaluation,,,,,,,,,,,,,\n",
        "102,Data science manager,Data Science Manager,,,\"1,000+\",Yes,classification,,Classification,no custom tools,No,,,scaling :),\"Slow,Confidence Requirements\",\"great for batch, terrible for real time\",\"Great for batch,Terrible for real time\",\"Open source model (GPT-J, etc)\",azure ml suite,Azure ML Suite,Embeddings,Embeddings,human verification of sample of inferences,Human Verification,we fine tune a bert sized model with our data and run the inference in batch. We experimented with quantization and distillation and its pretty impressive how much you can do with much smaller models.,Fine-tuned BERT model with quantization and distillation,,,,,,,,,,,\n",
        "103,Data Scientist,Data Scientist,,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,LLMs and their hallucinations just are not good for our current use cases (healthcare industry),Not good for current use cases in healthcare industry,,,\"I have not, but I know many of the devs want to; there are some other reasonable use cases too.\",No,\"Development, Other Use Cases\",,\n",
        "104,Principal Data Scientist,Principal Data Scientist,,,\"1,000+\",Yes,\"Knowledge graph, transcription, topic modeling, embedding everything\",,\"Knowledge Graph, Transcription, Topic Modeling, Embedding\",\"Yeah, but it's super secret until we try and sell it\",,,Secret project,Slow. They give bad answers when you set the confidence requirements too high,Compliance,Should we be paying for third party providers or train our own. Alpaca anyone?,\"Third party providers,Training our own\",Open AI API,\"Nvidia NeMO, Azure OpenAI\",\"Nvidia NeMO, Azure OpenAI\",Embeddings,Embeddings,Feedback button or manual review depending on use case,\"User Feedbacks, Manual Review\",Microsoft's call summarization tool can capture a surprising degree of detail while hilariously completely missing the main point of the meeting,Microsoft's call summarization tool captures detail but misses main point,Best practices to prevent leakage for client facing models that employ online learning?,What are the best practices to prevent leakage for client facing models with online learning?,,,,,,,,,\n",
        "105,Manager Machine learning,Machine Learning Manager,,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Less expertise,Less expertise,Large size models latency,Latency,,,,,\n",
        "106,Data scientist,Data Scientist,,,1-10,Yes,User facing chat application in a narrow domain.,,User-Facing Chat Application,In progress,,,In progress,,,,,Open AI API,,,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "107,Tech Lead,Tech Lead,,,\"1,000+\",Yes,,,,,,,,Compliance,\"Scalability,Immature Stack,Benchmarks,Risk Assessment\",\"Compliance, security and legal.\",\"Compliance,Security,Legal\",Open AI API,,,Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "108,Sr. Director AI/ML,,Senior Director,AI/ML,\"1,000+\",Yes,\"ITSM, Sales Prospecting\",IT,\"ITSM, Sales Prospecting\",chat bot,,,Chatbot,scalabiltiy,,data privacy,Data privacy,\"Open source model (GPT-J, etc)\",kubeflow,Kubeflow,Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "109,BDM,,BDM,,\"1,000+\",Yes,\"code development, summarisation\",,\"Code Development, Summarization\",\"development environment, pipelining and deployment tooling\",,,\"Development environment, pipelining and deployment tooling\",stack is immature; benchmarks are nonexistent; risk hard/impossible to gauge,,evaluating efficacy and ROI; understanding power needs/CO2 emissions,\"Evaluating efficacy,ROI,Power needs,CO2 emissions\",In house model,PyTorch + in-house tools,\"PyTorch, In-house Tools\",Inferenece for LLMs,Inference,user feedback,User Feedbacks,\"extremely power-hungry, hard to scale; networking bandwidth is as important as memory when parallelising\",\"LLMs are power-hungry and hard to scale, networking bandwidth is important\",is anyone measuring LLM inference power consumption (and CO2 emissions)? what are the observed ranges of watts/inference of the various model sizes?,Is anyone measuring LLM inference power consumption and CO2 emissions? What are the observed ranges of watts/inference of the various model sizes?,,,,,,,,,'''"
      ],
      "metadata": {
        "id": "TuaaFEaN9iUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REVIEWS = ''',What is your position/title at your company?,title,position,other,How big is your organization? (number of employees),Are you using LLM at in your organization?,What is your use case/use cases?,fields,tasks,Have you integrated or built any internal tools to support LLMs in your org? If so what?,integrated,solutions,purposes,What are some of the main challenges you have encountered thus far when building with LLMs,challenges,What are your main concerns with using LLMs in production?,concerns,how are you using LLMs?,What tools are you using with LLMs?,tools,What areas are you most interested in learning more about?,topics,How do you deal with reliability of output?,approaches,Any stories you have that are worth sharing about working with LLMs?,stories,Any questions you have for the community about LLM in production?,questions,What is the main reason for not using LLMs in your org?,reasons,What are some key questions you face when it comes to using LLM in prod?,challenges,have you tried LLMs for different use cases in your org?,tried,cases,\"If yes, why did it not work out?\",problems\n",
        "0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
        "1,Data scientist,Data Scientist,,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Production takes investment,Production Cost,Cost to maintain the service,Cost of Maintenance,Yes,Yes,,Date quality,Data Quality\n",
        "2,ML Engineer,Machine Learning Engineer,,,50-500,Yes,Text summarisation,,Text Summarization,No,No,,,\"Current infrastructure not designed for such massive models, having to implement workarounds for quick fixes\",\"Infrastructure,Workarounds\",Cost,Cost,\"Open source model (GPT-J, etc)\",Seldon,Seldon,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "3,Founder,Founder,,,1-10,Yes,Data annotation; summarization; search,,\"Data Annotation, Text Summarization, Search\",\"https://chat.mantisnlp.com, internal annotation tools\",No,MantisNLP,Internal Solution,\"out of date info, hallucinations, cost, difficulty in deploying on own infrastructure\",\"Accuracy,Outdated Information,Cost,Deployment\",they could serve nonsense or out of date information,\"Accuracy, Outdated Information\",Open AI API,,,Working with OSS LLMs,Open Source,we provide links to the context from which answers have been drawn.,References,,,,,,,,,,,,,\n",
        "4,Owner,Owner,,,1-10,Yes,Content production and brainstorming,,\"Content Creation, Brainstorming\",Not yet,No,,,not yet,,cost of openai API,Cost,Open AI API,\"Looking at Pinecone, weaviate, langchain, hugging face\",\"Pinecone, Weaviate, LangChain, Hugging Face\",Embeddings,Embeddings,temperature = 0,Temperature Control,,,,,,,,,,,,,\n",
        "5,Senior Principal Scientist,Principal Scientist,Senior,,\"1,000+\",Yes,Healthcare,Healthcare,,\"yes, confidential\",Yes,Confidential,,reliability,Reliability,cost of openai API,Cost,In house model,,,Working with OSS LLMs,Open Source,,,,,,,,,,,,,,,\n",
        "6,Head of Machine Learning,,Head,Machine Learning,50-500,No,,,,,,,,,,,,,,,,,,,,,,,It's only just now becoming feasible with the ChatGPT API. Before it was too expensive and quality was not good enough.,\"Cost, Quality\",How to make them reliable.,Reliability,\"We are still exploring, eg for support in diagnosis of plant diseases.\",Yes,Plant Diseases,So far it's not reliable enough and we don't have spent enough time preparing special content for it. Like rules it should use during diagnosis. The data ChatGPT was trained on seems a bit hit and miss for us.,Reliability\n",
        "7,Senior Machine Learning Engineer,Machine Learning Engineer,Senior,,\"1,000+\",Yes,\"Search, classification, NER\",,\"Search, Text Classification, Named Entity Recognition\",Just a bunch of shitty scripts I need to refactor in the future.,Yes,,,\"Size in all regards. Also training time, response time and that multi gpu debugging is weird.\",\"Size,Training,Response Time,Debugging\",\"Whether the cost involved for us justifies the gains we get, considering we mainly make money from ads and not a really good product.\",\"Cost, Business Value\",Other model provider API,\"Hugginface, sagemaker, openapi api, tensorflow.\",\"Hugging Face, SageMaker, OpenAPI, TensorFlow\",Working with OSS LLMs,Open Source,\"Ill refer to the picture of the xkcd guy \"\"just stir the model until the numbers look right\"\".\",Manual Tuning,\"I am not sure what ranks as a LLM. Anything bert and beyond? Anyway, we once used setfit for a production task but found that performance was really tricky to debug. The proof of concept was fine. Production was a complete mess. To this day I dont know why- underlying hardware?\",Production issues with LLMs,I mainly wonder how everyone is monitoring these. ALSO. For my company GDPR is a big deal. My DPO shoots down our ideas more often than not or limits it to using the last three months of data. How do other people deal with this?,How do other people monitor LLMs and deal with GDPR limitations?,,,,,,,,,\n",
        "8,Head of Product,,Head,Product,10-50,No,,,,,,,,,,,,,,,,,,,,,,,No applications,Lack of Use Case,ROI of more expensive models,ROI,Yes,Yes,,No ROI,ROI\n",
        "9,,,,,50-500,Yes,,,,oi,No,,,lik,,il,,Open AI API,loi,,Working with OSS LLMs,Open Source,lo,,lo,,lo,,,,,,,,,,\n",
        "10,Senior Data Engineer,Data Engineer,Senior,,50-500,Yes,\"text embeddings for search, text generation to help the business people do their job\",\"Business, Search\",\"Text Embeddings, Text Generation\",\"Yes, we're using OpenAI for a couple of internal tools.\",Yes,OpenAI,Internal tools,,,Cost. But our issues with productionisation is more general than just for LLMs,\"Cost, Productionization\",Open AI API,streamlit,Streamlit,Inferenece for LLMs,Inference,\"All internal tools, reliability is not a major concern for now\",Nothing,,,,,,,,,,,,,\n",
        "11,Machine learning engineer,Machine Learning Engineer,,,\"1,000+\",Yes,\"Natural Language interface, domain specific content creation\",,\"Natural Language Interface, Content Creation\",,,,,Prompt debugging! We need a new form of debugger.,Debugging,They are erratic,Reliability,Open AI API,,,Working with OSS LLMs,Open Source,We have control systems,Control Systems,,,,,,,,,,,,,\n",
        "12,Senior Data Scientist,Data Scientist,Senior,,\"1,000+\",Yes,Chatbots and Text classification,Customer Service,\"Chatbots, Text Classification\",One of our use cases is to build text classification models for each of our customers (we are a B2B company). Achieved this using LLMs and some engineering,Yes,,Text Classification,,,,,\"Open source model (GPT-J, etc)\",,,Embeddings,Embeddings,,,,,,,,,,,,,,,\n",
        "13,ML Engineer,Machine Learning Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
        "14,Snr MLOps Engineer,MLOps Engineer,Senior,,50-500,Yes,Adapters,,Adapters,\"Yes, training setup as well as inference setup\",Yes,,,Latency and deployment time,\"Latency,Deployment\",Resources usage along with latency and deployment time,\"Resource Usage, Latency, Deployment\",\"Open source model (GPT-J, etc)\",,,Inferenece for LLMs,Inference,Assessment at evaluation,Model Evaluation,,,,,,,,,,,,,\n",
        "15,,,,,\"1,000+\",Yes,,,,,,,,,,,,,,,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "16,Senior ML engineer,Machine Learning Engineer,Senior,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,\"Two main reasons: 1. Very compute & power hungry 2. Cannot be relied upon as it will very confidently return incorrect information or in ‚Äúexistent facts‚Äù which is problematic for use in my industry.\n",
        "\n",
        "Eg it took us less than 3mn to get BioGPT to tell us vaccines cause autism\",\"Compute, Reliability\",\"What uses cases does it have for our business? how much gain do we get from using an LLM vs a simpler language model where we can control fact and outputs? How much will it cost (to deploy, and run)\",\"Business Use Cases, Cost\",We abandoned the idea pretty quickly,Yes,,We got terrible suggestions from the LLM that were false information and would be detrimental to the business,\"Veracity, Business Risks\"\n",
        "17,Machine Learning Engineer,Machine Learning Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,\"We use medium-sized language models (e.g. RoBERTa-base) as the backbone for our deployed NLP models because they can perform inference on CPU, which is cheap. We don't use LLMs (e.g. ChatGPT, GPT-3) because the APIs are expensive, and fine-tuning them to perform NLP tasks other than language generation (e.g. NER, open-set intent classification) is not straightforward.\",\"Cost, API Limitations, Fine-tuning Difficulty\",How can we ensure the same version of the model is being used? How can we supply a random seed to get deterministic results? How can we minimize LLM API usage costs? How can we fine-tune a LLM on a niche task without having to replicate the weights of the LLM? How can we deploy our own copy of an open-source LLM without breaking the bank?,\"Version Control, Random Seed, API Usage Costs, Fine-tuning, Deployment Cost\",Yes,Yes,,\"We evaluated a LLM (GPT-3) against RoBERTa fine-tuned on our small task-specific dataset, and our fine-tuned model got much higher accuracy than GPT-3 did.\",\"Cost, Efficiency\"\n",
        "18,,,,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Not much usecases for us,Lack of Use Case,,,,,,,\n",
        "19,Founding Data Scientist,Data Scientist,Founding,,1-10,Yes,\"Summarization, topic extraction, root-cause analysis\",,\"Text Summarization, Topic Extraction, Root-Cause Analysis\",\"Not much, just some prompt storage and an automated retries to deal with different failure cases.\",No,,,\"Hallucinations are a big one. In general, problems are hard to debug without careful manual review, so we're only using them when the amount of results generated is small enough to review by hand.\",\"Debugging,Hallucinations\",\"Reliability, cost, injecting domain-specific language and understanding into them\",\"Reliability, Cost, Domain-specific Language\",Open AI API,,,Embeddings,Embeddings,\"Automated retries for some simple & obvious failures, manual review for everything else\",\"Automated Retries, Manual Review\",,,Where are others drawing the line between training models locally and using LLMs? Have others had success with using LLMs to bootstrap training data for smaller local models?,Where is the line drawn between training models locally and using LLMs? Can LLMs be used to bootstrap training data for smaller models?,,Boostrap Training,,,,,,,\n",
        "20,CTO,CTO,,,1-10,Yes,Marketing,Marketing,,Not yet,No,,,Still developing,,Cost,Cost,Open AI API,,,Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "21,Machine Learning Engineer,Machine Learning Engineer,,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,Not applicable to the domain,Not Applicable,Not yet,,,,,,\n",
        "22,Head of AI,,Head,AI,\"1,000+\",Yes,Scanning large text corpora using an LLM and some cases with text summary for now,,\"Text Scanning, Text Summarization\",Not (yet).,No,,,It‚Äôs a mix of taming a beast and using alchemy,,Consistency of the output quality,Consistency,Other model provider API,./.,,Working with OSS LLMs,Open Source,HITL,Human-in-the-Loop (HITL),They take every Input equally serious which makes for some great laughs‚Ä¶,LLMs provide entertainment value,,,,,,,,,,,\n",
        "23,Product,,,Product,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Production LLM lifecycle and use cases not clear,\"Production Lifecycle, Lack of Use Case\",Data governance,Data Governance,no,No,,,\n",
        "24,Lead Data Scientist,Data Scientist,Lead,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,Cost / right use-case,\"Cost, Lack of Use Case\",,,,,,,\n",
        "25,Co-founder,Co-Founder,,,1-10,Yes,AI for e-commerce,E-commerce,,No,No,,,Hardware requirements and AI accelerators compatibility,\"Hardware,Compatibility\",Inference scalability,Scalability,\"Open source model (GPT-J, etc)\",\"PyTorch, AWS SageMaker, OpenAI's API, Midjourney, CLIP/BLIP/etc models\",\"PyTorch, SageMaker, OpenAI, Midjourney, CLIP, BLIP\",Inferenece for LLMs,Inference,,,,,\"Any opinions regarding the newer AI accelerator chips? Many people fighting for GPUs, but not necessarily mentioning AWS Inferentia 2 chips, TPUs, Habana Gaudi, etc.\",\"What are opinions on newer AI accelerator chips like AWS Inferentia 2 chips, TPUs, Habana Gaudi, etc.?\",,,,,,,,,\n",
        "26,MLE,Machine Learning Engineer,,MLE,50-500,Yes,,,,dbt pre-processing to get the text right for the prompts and test it,No,DBT,Prompt testing,\"explainability, expense, embeddings\",\"Explainability,Expense,Embeddings\",see above,,Open AI API,metaflow,Metaflow,Embeddings,Embeddings,,,,,,,,,,,,,,,\n",
        "27,Snr MLE,Machine Learning Engineer,Senior,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,Exploration stage,Exploration Stage,Putting company data in the right structure and updating the model with it,Data Preparation,yes,Yes,,getting fine tune right and LLMops pipeline,\"Fine Tuning, LLMOps\"\n",
        "28,Director of Artificial Intelligence,,Director,Artificial Intelligence,\"1,000+\",Yes,\"Novelty - ChatGPT is being explored to optimize code, critique written works, etc.\",,\"Code Optimization, Written Work Critique\",No,No,,,,,Complacency and atrophy of critical thinking,Critical Thinking,\"Open source model (GPT-J, etc)\",Microsoft Azure OpenAI services,\"Microsoft Azure, OpenAI\",Fine tunning LLMS,Fine-Tuning,Educate others on the limitations and proper use.,Education,\"One of our citizen data scientists now (figuratively) \"\"worships\"\" ChatGPT Gods given its capabilities. LOL.\",Citizen data scientist impressed with ChatGPT capabilities,None at the moment given our priorities are mostly with computer vision and time-series data analysis.,,,,,,,,,,\n",
        "29,Data Scientist,Data Scientist,,,50-500,Yes,\"Summarisation, Email generation, Information retrieval\",,\"Text Summarization, Email Generation, Information Retrieval\",Modular python library for interacting with multiple LLM providers/APIs and a standardized prompt generation framework.,No,\"Python, API\",Prompt Generation,Consistency of formatting in freeform LLM text generation. Subtleties in prompt engineering.,,\"Detecting truthfulness, hallucination. Output validation. Risk of monopolisation and centralised access to LLM as a service\",\"Truthfulness, Hallucinations, Validation, Monopolization\",Other model provider API,Python,Python,Working with OSS LLMs,Open Source,,,,,,,,,,,,,,,\n",
        "30,Co-founder and CTO,\"Co-Founder, CTO\",,,1-10,Yes,Prompt autogeneration and synthetic data generation,,\"Prompt Autogeneration, Synthetic Data Generation\",,,,,Reproducibility,\"Formatting,Prompt Engineering\",Hallucination and latency,\"Hallucination, Latency\",Open AI API,HoneyHive,HoneyHive,Fine tunning LLMS,Fine-Tuning,\"Multi-responses shown to user side by side, ghost writer style dropout\",\"Multi-Responses, Ghost Writing\",,,How are you thinking about tracking user feedback in production?,How to track user feedback in production?,,,,,,,,,\n",
        "31,,,,,50-500,Yes,,,,,,,,,Reproducibility,,,,,,Working with OSS LLMs,Open Source,,,,,,,,,,,,,,,\n",
        "32,Senior Software Enginner,Software Engineer,Senior,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,No business need yet,Lack of Use Case,,,,,,,\n",
        "33,Co-founder,Co-Founder,,,1-10,No,,,,,,,,,,,,,,,,,,,,,,,We're learning,,,,,,,,\n",
        "34,ML Developer,Machine Learning Developer,,,\"500-1,000\",Yes,Question Answering,,Question Answering,yes,Yes,,,latency,Latency,\"latency, factuality\",\"Latency, Factuality\",\"Open source model (GPT-J, etc)\",,,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "35,,,,,1-10,No,,,,,,,,,,,,,,,,,,,,,,,\"Cost, Latency, Accuracy\",\"Cost, Latency, Accuracy\",\"Cost can be high, hallucinations\",\"Cost, Hallucinations\",,,,,\n",
        "36,data scientist,Data Scientist,,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,not really in our domain,Not Applicable,,,no - closest we've got to dealing with language is fuzzy lookups,No,,,\n",
        "37,Senior Machine Learning Engineer,Machine Learning Engineer,Senior,,10-50,Yes,Question generation for accountants,Accounting,Question Generation,built internal,Yes,,,reducing latency,Latency,not knowing the data used for the initial training,Data Transparency,In house model,\"TensorFlow, TFX\",\"TensorFlow, TFX\",Inferenece for LLMs,Inference,yes,,,,,,,,,,,,,,\n",
        "38,Founder,Founder,,,1-10,Yes,Zero-shot/Few Shot classification for data enrichment,,\"Zero-shot/Few Shot Classification, Data Enrichment\",\"Internally, we use BERT to create embeddings, then train a smaller model on customer data.\",Yes,BERT,Embeddings,Infrastructure costs.,Cost,\"Is what I;m building getting obsolete in 3 months,\",Obsolescence,In house model,In house Bert (from Huggingface) + Open AI,\"In House, Bert, HuggingFace, OpenAI\",Fine tunning LLMS,Fine-Tuning,I am stuck with this right now - can I integrate these into the UI for AI Hero.,UI Integration,ChatGPT API output is different every time for the same input - makes it hard to create a reliable service.,Reliability with ChatGPT API is hard,How are you managing training/deployment of these?,How to manage training/deployment of LLMs?,,,,,,,,,\n",
        "39,SRE,Site Reliability Engineer,,,\"1,000+\",Yes,analyze logs,,Log Analysis,no,No,,,Analyzing results,Result Analysis,security,Security,In house model,in house built tool,In House,Fine tunning LLMS,Fine-Tuning,Very manual so far.,Manual Review,confidential,Confidentiality,no,,,,,,,,,,\n",
        "40,\"Cofounder (normally product manager, but also wearer of many hats)\",Co-Founder,,Product Manager,1-10,No,,,,,,,,,,,,,,,,,,,,,,,\"1. gcm diagflow is good for complete basic dialogs, 2. halucinations 3.a. how to make answers be focused on what my company is doing rather than general answers, 3.b. how to keep that evolving as we learn more about the customers 3.c. how to prevent a practical joker who's conversing with my llm to inject some practical joke into it 4. if im relying on openai-s llm, i need to always get a reply what happens if they're busy, whats the proverbial 'failover'\",\"Simpler Tools, Hallucinations, Fine-Tuning, Model Regeneration, Security, Fail-over\",how to get it to use my company's specific knowledge,Domain Specific Knowledge,not thoroughly enough yet,No,,still working on it,\n",
        "41,Data Architect,Data Architect,,,\"1,000+\",Yes,Search and chat on proprietary data,,\"Search, Chatbot\",No,No,,,\"Automation amd scaling of transfer learning, tuning and reinforcement learning\",\"Automation,Scaling,Transfer Learning,Tuning,Reinforcement Learning\",Scaling,Scalability,Open AI API,\"Lang chain, k8s, gcp\",\"LangChain, Kubernetes, GCP\",Learning from new/private data,Fine-Tuning,Not a concern. In-house,Nothing,No,,\"Best automated/scalable pipeline for tuning,, transfer learning (augmentation), reinforcement feedback\",\"What is the best pipeline architecture for LLM tuning, transfer learning, and reinforcement feedback?\",,,,,,,,,\n",
        "42,Senior software engineer,Software Engineer,Senior,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,No need atm,Lack of Use Case,Reliability,Reliability,,,,,\n",
        "43,consultant,Consultant,,,1-10,No,,,,,,,,,,,,,,,,,,,,,,,we'll be using at a startup I'm part of,,cost and fit for purpose. Actual market validation,\"Cost, Market Validation\",yes. I've used them for classification,Yes,Classification,they are in prod for that use case,\n",
        "44,CTO,CTO,,,50-500,Yes,\"Product (Software) recommendations, Internal Q&A, Marketing content, and Customer support\",\"E-commerce, Customer Support, Marketing\",\"Recommendation, Question Answering, Text Generation\",\"Langchain, OpenAI Playground, ChatGPT, PromptLayer, Weaviate\",No,\"Langchain, OpenAI Playground, ChatGPT, PromptLayer, Weaviate\",,Rapidly changing APIs,API changes,Hallucinations misleading customers,\"Hallucinations,Misleading customers\",Open AI API,PromptLayer,PromptLayer,Embeddings,Embeddings,\"Pre/post-processing, constitutional / self-auditing, customer feedback thumbs up/down, custom embedding matrix\",\"Pre/Post-Processing, Self-Auditing, Customer Feedback, Custom Embedding Matrix\",,,How might you avoid siloing all of your embeddings when different teams need different tools without building it all in house yet all would benefit from the combined collection?,How to avoid siloing embeddings when different teams need different tools without building it all in house?,,,,,,,,,\n",
        "45,Data scientist / co-founder,\"Data Scientist, Co-Founder\",,,1-10,Yes,Information retrieval and text generation.,,\"Information Retrieval, Text Generation\",\"We have some output validation methods and templates, basically regex matching prompts.\",Yes,,Output validation,Input length limitation and having to work around it with non-optimal methods.,Input length limitation,Hallucinations,Hallucinations,Open AI API,\"Langchain, Huggingface, Polars\",\"LangChain, HuggingFace, Polars\",Fine tunning LLMS,Fine-Tuning,We implement checking with less stochastic tools.,Stochastic Tools Checking,Content language matters a lot!,Content language is important for LLMs,,,,,,,,,,,\n",
        "46,Analytics engineer,Analytics Engineer,,,50-500,Yes,Chatbot,,Chatbot,In POC at the moment,Yes,,POC,Vectorizing CRUD and ci / CD,\"CI/CD,Performance\",To get better at semantic search,Semantic search,Open AI API,\"Langchain, pinecone, slack and aws\",\"LangChain, Pinecone, Slack, AWS\",Embeddings,Embeddings,,,,,\"How to better deploy python apps using aws and operationalizing it.\n",
        "How to better manage vector indexes.\",How to deploy and operationalize python apps using AWS?,,,,,,,,,\n",
        "47,\"Director, Innovation\",,Director,Innovation,50-500,No,,,,,,,,,,,,,,,,,,,,,,,Data privacy concerns,Data Privacy,Difficulty or ease of maintenance; costs; data privacy,\"Maintenance, Costs, Data Privacy\",Not yet,No,,na,None\n",
        "48,ML & MLOps practice lead,,Lead,ML & MLOps,50-500,Yes,\"Text classification, text generation\",,\"Text Classification, Text Generation\",,,,,\"Compute performance, fine tuning challenges, explainability\",\"Performance,Explainability,Fine-tuning\",,,\"Open source model (GPT-J, etc)\",,,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "49,Founder,Founder,,,1-10,Yes,Social media idea generation,Social Media,Text Generation,No just used langchain and OpenAI,No,\"Langchain, OpenAI\",,Prompt engineering; handling very large documents,\"Prompt engineering,Handling large documents\",Bad data/completions; runtime errors parsing completions (we use regex to parse each line ),\"Bad data,Runtime errors\",Open AI API,LangChain,LangChain,Fine tunning LLMS,Fine-Tuning,We don‚Äôt do anything right now,Nothing,,,,,,,,,,,,,\n",
        "50,CTO,CTO,,,1-10,Yes,Semantic data extraction and corporate governance,Corporate Governance,Semantic Data Extraction,\"I'm unable to utilise LLM's currently due to data security restrictions, however as a OpenAI beta tester, around 2 years ago I created a business Operational Delivery Framework for a ¬£500M nuclear project. I then transferred the learning framework to ¬£5B project.\",No,,Operational Delivery Framework,Environment dependency conflicts,Environment dependency conflicts,Safe and ethical usage,\"Safety,Ethics\",Open AI API,Use case dependent,,Fine tunning LLMS,Fine-Tuning,\"AI is replaced by HI(Human) for final check, always.\",Human-in-the-Loop (HITL),\"Yes, the ODF I created was regarded as best in class by regulators and has been suggested to be implemented as new gold standard. I used GPT3 to advise on the design of the framework, and to write all the necessary code to create it.\",GPT3 used to create best in class ODF framework,\"Nope, no questions, just a great big thankyou, and a virtual high five for all the awesome peeps üôè\",,,,,,,,,,\n",
        "51,MLOps Engineer,MLOps Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,Still evaluating infrastructure requirements and ways to make it traceable if things go wrong,,\"Retraining, tackle hallucinations and avoid wrong results\",\"Retraining, Hallucinations, Accuracy\",No,No,,,\n",
        "52,Junior developer,Developer,Junior,,1-10,Yes,Creating a chat bot to query data efficiently for managers,,\"Chatbot, Information Retrieval\",Not yet but its in production. Using langchain.,Yes,Langchain,Production,\"The main challenges are using the different tools from langchain to optimise for the use case. There are a lot of things to consider. Also, we use ts and angular front end so not having all the tools that are available in python required us to have python as the backend now.\",\"Optimization,Interoperability\",It‚Äôs how useful/valuable it will actually be for the client.,Customer Value,Open AI API,Langchain/ts/csharpe/angular.,\"LangChain, TypeScript, C#, Angular\",Fine tunning LLMS,Fine-Tuning,We havent gotten to that yet.,Nothing,\"They are amazing. But as mentioned, a chatbot alone is probably not enough to be useful, hence building out other capabilities will become important.\",Integrations are important for chatbot,,,,\"Infrastructure Requirement, Traceability\",,,,,,,\n",
        "53,deep learning engineer,Deep Learning Engineer,,,\"1,000+\",Yes,extract information from image and text of medical domain,Medical,\"Information Extraction, Image Analysis, Text Analysis\",no,No,,,deploy models into production,Deployment,hight training & inference cost,\"Training cost,Inference cost\",\"Open source model (GPT-J, etc)\",\"transformers, triton inference server\",\"Transformers, Triton Inference Server\",Working with OSS LLMs,Open Source,,,,,,,,,,,,,,,\n",
        "54,Co-Founder,Co-Founder,,,10-50,No,,,,,,,,,,,,,,,,,,,,,,,Lack of use cases that require so much sophistication,Lack of Use Case,\"cost of maintenance, lack of institutional knowledge\",\"Cost of Maintenance, Institutional Knowledge\",No,No,,,\n",
        "55,Intern,,Intern,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Because I am in education sector.,Not Applicable,when you use LLM in production it can fail any time,Reliability,NO,No,,,\n",
        "56,CTO,CTO,,,1-10,Yes,Coding,,Coding,VSCode Extension,Yes,,VSCode Extension,Work with 3 AI providers at the same time,Interoperability,The wrong respond from the model,Incorrectness,Open AI API,Code GPT https://codegpt.co,Code GPT,Embeddings,Embeddings,Now it is just responds tickets in github,GitHub Tickets,\"The extension hace more than 200,000 downloads and it work great!\",LLM leads to great success,,,,,,,,,,,\n",
        "57,Software Development Engineer,Software Development Engineer,,,\"1,000+\",Yes,\"(Not in my org, at work I don‚Äôt do this, it‚Äôs personal) Cover letter generator based on resume and job description\",,Text Generation,,,,,Embedding with appropriate data effectively,Embedding,Being dependent on one company for the API so far. Having models of equivalent performance whose weights were public would be a great relief,Vendor Lock-in,Open AI API,\"Gpt index, langchain\",\"GPT Index, LangChain\",Embeddings,Embeddings,\"Prompt engineering, allow user to be specific, and of course re-generate\",\"Prompt Engineering, User Feedback, Regeneration\",,,,,,,,,,,,,\n",
        "58,Data Science Team Lead,Data Science Team Lead,,,10-50,Yes,Topic sentiment evaluation on text,,\"Sentiment Analysis, Topic Modeling\",,,,,Managing training data labeling,Data labeling,explainability,Explainability,\"Open source model (GPT-J, etc)\",AWS Sagemaker,AWS Sagemaker,Fine tunning LLMS,Fine-Tuning,Test each new model candidate on test set,Model Evaluation,,,,,,,,,,,,,\n",
        "59,Sr. ML Eng,Machine Learning Engineer,Senior,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,inference cost vs. impact on revenue,Return of Investment,how to reduce inference cost,Cost,,,,,\n",
        "60,Senior AI Expert,AI Expert,Senior,,50-500,Yes,Medical Diagnostics Support,Medical,Diagnostics Support,\"No, we rely on open source tools\",No,,,\"Explainability, Testing, Getting ground truth data\",\"Explainability,Testing,Data labeling\",Trustworthiness,Trustworthiness,\"Open source model (GPT-J, etc)\",\"Milvus DB, FastAPI, Hugginface, Pytorch, etc.\",\"Milvus DB, FastAPI, Hugging Face, PyTorch\",Working with OSS LLMs,Open Source,We try to make the output as transparent as we can (using explainability techniques) and not relying upon LLM for the end decision (these are used as a support tool),\"Model Explainability, Human Decision Making\",,,,,,,,,,,,,\n",
        "61,Data Science Engineer,Data Science Engineer,,,50-500,Yes,Assistant chat bot,,Chatbot,\"Langchain. Will serve the bot from a flask server, being set up for the bot, into our existing java platform\",Yes,Langchain,Chatbot,\"Langchain LLM Parse error. Turbo not using multiple tools for some reason, when davinci-003 did\",Parse error,\"Need HIPAA compliance, so trying to get on Azure. Worried about the contract falling through\",\"HIPAA Compliance,Contracts\",Open AI API,Langchain. All my agent‚Äôs tools are custom-built. I hit our internal APIs with them,LangChain,I want deep knowledge on how to make the ReAct pattern work in various edge cases,\"React Pattern, Edge Cases\",\"Will have friendly internal users using it first in the course of serving customers, and giving us per-message feedback\",User Feedback,davinci-003 hallucinating using an existing tool without actually using it!,LLM hallucination without actual usage,How should I split my initial prompt up between turbo‚Äôs System prompt and the first Human prompt I give it?,How to split initial prompt between turbo's System prompt and the first Human prompt?,,,,,,,,,\n",
        "62,Head of Data and ML,,Head,Data and ML,10-50,No,,,,,,,,,,,,,,,,,,,,,,,\"No Use case, being a fintech startup, besides helping with processing documents\",Lack of Use Case,why waste the money,Cost,no,No,,,\n",
        "63,ML/MLOps Engineer,ML/MLOps Engineer,,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,No clear use case,Lack of Use Case,\"Running them, fine tuning, cost\",\"Running, Fine-tuning, Cost\",Nope,No,,,\n",
        "64,\"Data Scientist, HappyFresh\",Data Scientist,,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,Not,,,,,,,,\n",
        "65,Data Scientist,Data Scientist,,,10-50,Yes,Automation of report writing.,,Report Generation,\"Yes.\n",
        "Responsible for the design, building, testing and deployment of a GPT-3.5 application that processes numerical data and free-response text in tabular form to generates commentary and analysis for a visual report. Researchers are only required to upload a zip folder(1-click).\n",
        "Technologies: AWS lambda, Docker (ECR/ECS), GPT3.5, PyTorch.\",Yes,,,\"control of the output for our usecase. Reports need to understand the client's intent and take information from briefing calls and key erquriements. These can help guide the output, but now always.\",Output control,Thta it will hallucinate something that we won;t pick up in the report editing phase,Hallucinations,Open AI API,Torch and Pandas,\"Torch, Pandas\",Prompt Engineering (Riley Goodside),Prompt Engineering,Have editors comb through it,Manual Review,We aim to turn a report around from survey finishing to client inbox in 24 hour. So far we are at 48 hours.,Development time is under-estimated,,,,,,,,,,,\n",
        "66,Data Scientist,Data Scientist,,,50-500,Yes,\"LLM in Marketing, Custom data\",\"Marketing, Custom Data\",Text Generation,HF Inference Endpoints,Yes,HuggingFace,\"Chatbot, Question Answering\",Cloud Cost,Cost,No proper documentation,Documentation,Other model provider API,,,Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "67,Senior Data Scientist,Data Scientist,Senior,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,\"We don't have a need for generating unstructured data, we work in small languages, our text is very domain specific, LLMs are very compute intensive -> expensive to run and take a lot of work to set up.\",\"Domain Specific Language, Compute Intensive, Cost\",\"Cost, latency, actual performance in the languages / domains we work with (real languages, not programming)\",\"Cost, Latency, Accuracy\",Nope. Tried some BERT for classification but that hardly counts as an LLM. And I've made models as big as BERT I think.,No,,,\n",
        "68,Data Scientist,Data Scientist,,,50-500,Yes,chatbot/voice assistant,,\"Chatbot, Voice Assistant\",just used RASA for chatbot creation and haystack for open domain question answering,Yes,\"RASA, Haystack\",\"Chatbot, Question Answering\",\"currently used only pre-trained models, biggest challenge was to obtain data for specific domain fine tuning\",Data acquisition,it is very important in my opinion to accurately monitor the kind of usage to prevent frauds or malicious intentions,\"Usage monitoring,Fraud prevention\",Other model provider API,Haystack,Haystack,Fine tunning LLMS,Fine-Tuning,\"it depends from specific context and specific use cases, in my experience I rely on validation set and a validation tool that estimates accuracy of answers based on answers manually scrutinized by humans\",\"Validation Set, Validation Tool, Human Scrutiny\",not so far,,it would be interesting a discussion about how to control the output and in general the kind of usage the system has been exposed,How to control the output and usage of LLMs?,,,,,,,,,\n",
        "69,Head of Data Science,,Head,Data Science,\"1,000+\",Yes,\"Entity matching, customer service responses (souped up/targetted FAQ).\",Customer Service,\"Entity Matching, Text Classification\",yes - high scale server on K8's,Yes,Kubernetes,,K8's caches... TESTING.,Testing,\"cost, latency, truthfulnes - but also\n",
        "\n",
        "Maybe management in the sense of versioning and resolving / attributing issues?\n",
        "\n",
        "Copyright and intellectual property liability - who knows what it's been trained on and what it might spit out... What happens if it provides a Disney script for an advert? (I know the answer - nothing good).\n",
        "\n",
        "Safety : what if it tells someone to kill themselves? Or spits out a recipe to make smallpox? (I am being kind of silly here, but... not entirely)\",\"Cost,Latency,Truthfulness,Versioning\",\"Open source model (GPT-J, etc)\",\"not sure what this means - we use k8's and cloud functions to run them, also interface to case management solutions.\",Kubernetes,calibration of llm output / calbration and testing of llms,\"Calibration, Testing\",badly. We do lots of testing but I constantly feel we are on shaky ground.,Extensive Testing,\"We had a nice time selecting models, proving it would all work and using TSNE and PCA to reduce the dimensionality of embeddings and get compression for our vector lookup system... Very good and satisfying work, we got prizes and praising from all. Then camesome dreadful times making our vector lookup system work under load for long durations in a proper prod environment (where we couldn't touch it to do things like restart it or reload the index). I think this will be a big issue in the future - we need things that are as reliable as oracle for running the inferences, we do not have them - everything is glued together (I am guessing this is not true if you work for Google...)\",Scalability is a real challenge,,,,,,,,,,,\n",
        "70,Founder,Founder,,,1-10,Yes,We are making tools to monitor and improve the performance of LLMs,,\"Performance Monitoring, Model Improvement\",No additional tools,No,,,,,Hallucinations,Hallucinations,\"Open source model (GPT-J, etc)\",UpTrain,UpTrain,Embeddings,Embeddings,We are building a tool for that,Tool Development,,,How is everyone thinking about fine-tuning LLMs for their use-cases?,How to fine-tune LLMs for specific use-cases?,,,,,,,,,\n",
        "71,CEO,CEO,,,1-10,Yes,\"Embeddings, Feature Extraction, Chatbot\",,\"Embeddings, Feature Extraction, Chatbot\",Database connections,No,,,Context length limitations,Context length limitation,\"Hallucinations, rate limiting,\",\"Hallucinations, Rate Limitations\",Open AI API,\"Langchain, Llama Index, Deep lake, Weaviate\",\"LangChain, Llama Index, Deep Lake, Weaviate\",Fine tunning LLMS,Fine-Tuning,Error handling,Error Handling,,,,,,,,,,,,,\n",
        "72,Founder,Founder,,,1-10,Yes,\"Text summarization, code generation\",,\"Text Summarization, Code Generation\",No,No,,,Dealing with vector stores,Vector stores,Keeping agent on track with system (OpenAI‚Äôs ChatGPT) or role,Consistency,Open AI API,LangChain,LangChain,Embeddings,Embeddings,,,,,What are some of the best practices for dealing with LLMs or foundation models?,What are the best practices for dealing with LLMs or foundation models?,,,,,,,,,\n",
        "73,Senior Dara scientist,Data Scientist,Senior,,\"1,000+\",Yes,\"Chatbot, semantic search\",,\"Chatbot, Semantic Search\",Deploy open source LLM,Yes,,,\"Data privacy, big models\",\"Data privacy,Model size\",Fake information,Fake information,\"Open source model (GPT-J, etc)\",PyTorch huggingaface,\"PyTorch, Hugging Face\",Inferenece for LLMs,Inference,Generate multiple output,Multiple Output Generation,Not really,,Chatgpt for domain knowledge,How to use Chatgpt for domain knowledge?,,,,,,,,,\n",
        "74,ML Engineer,Machine Learning Engineer,,,\"1,000+\",Yes,Customer support,Customer Support,,We‚Äôve integrated with model store!,Yes,,,Memory requirements during inference,\"Memory,Inference\",Slow start up times/it‚Äôs very different from regular code,\"Slow start up times,Different from regular code\",In house model,Hugging Face,Hugging Face,Inferenece for LLMs,Inference,We‚Äôre doing classification not generation,Proxy Task,,,Curious to hear about other use cases!,What are other use cases for LLMs?,,,,,,,,,\n",
        "75,Director,,Director,,1-10,No,,,,,,,,,,,,,,,,,,,,,,,No need yet,Lack of Use Case,I don't need them,,\"Yes, too heavy\",Yes,,Too heavy,Model Size\n",
        "76,Technical Marketing Manager,Technical Manager,,Marketing,10-50,Yes,Content marketing and text drafting,Marketing,Text Generation,not at the moment in my department,No,,,,,costs under control and reliability,\"Costs,Reliability\",\"Open source model (GPT-J, etc)\",,,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "77,Head of MLOps,,Head,MLOps,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,\"Currently assessing the risk and potential applications, performing POCs.\",Business Risks,\"What are the potential risks, where are the benefits worth the costs for enterprise grade integration, how do we make these things secure, how do we monitor?\",\"Security, Monitoring, ROI\",In the process of doing so.,Yes,,We‚Äôre being thorough.,Thoroughness\n",
        "78,CDO,CDO,,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Wee are exploring the space but don't feel they are ready for production use yeet,Not Production Ready,\"How to pin them to a domain, how to verify the veracity of teh outcome and how to best chain them with APIs\",\"Domain Specific Knowledge, Outcome Verification, API Integration\",We are exploring customer facing and colleague facing applications,Yes,Customer and Colleague Facing Applications,Concerns about sticking to a domain and about veracity if the answers,\"Domain-specificity, Veracity\"\n",
        "79,Owner,Owner,,,1-10,Yes,\"sales pipelines, text generation, data classification and many more\",Sales,\"Text Generation, Data Classification\",\"Yes, several\",Yes,,,The need to use Python,Python,\"Cost, accuracy\",\"Costs,Accuracy\",Open AI API,Zapier,Zapier,Finetuning and embeddings,\"Fine-Tuning, Embeddings\",extensive testing,Extensive Testing,,,,,,,,,,,,,\n",
        "80,Agricultue consultant,Consultant,,Agriculture ,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,$,Cost,$,Cost,No,No,,No,None\n",
        "81,Data scientist,Data Scientist,,,1-10,Yes,,,,Yes,Yes,,,Computing power,Computing Power,,,Open AI API,,,Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "82,Product Owner Data Science,Product Owner,,Data Science,50-500,No,,,,,,,,,,,,,,,,,,,,,,,MLOps and DataScience rather young discipline. We are currently working on an LLM use case though,Lack of Maturity,\"Runtime, Drift detection\",\"Runtime, Drift Detection\",No,No,,,\n",
        "83,Team Lead DS & MLE,Team Leader,Lead,DS & MLE,50-500,No,,,,,,,,,,,,,,,,,,,,,,,Domain specific challenges for what we do. Digging into how we could train LLM to fit the needs.,\"Domain specific challenges,Fine Tuning\",What is it? There is just the huge gap in understanding about analyzing text vs generating text.,\"Understanding, Analyzing vs Generating Text\",We were using s-BERT in our primary recsys model. Recent modeling efforts have reset to simpler systems that are performing better. Still using it in a 2nd model for a clustering challenge.,Yes,\"Recommendation System, Clustering\",,\n",
        "84,Co-founder,Co-Founder,,,1-10,Yes,Semantic vector embeddings as a feature into RecSys,Recommender Systems,Semantic Vector Embeddings,Not yet,No,,,\"Sparse representations (e.g. tfidf) are easier to work with for our use-cases, because they survive simple aggregation better.\",Sparse Representation,Explainability of embeddings.,Explainability,\"Open source model (GPT-J, etc)\",,,Embeddings,Embeddings,We have come up with a large list of proxy metrics that we found correlate with performance on our tasks - so we track those.,Proxy Metrics,\"As far as embeddings are concerned, we haven't found a clear advantage of LLMs vs simpler models (tfidf) trained on a relevant dataset. Both require elaborate preprocessing of the embedded documents for example.\",No clear advantage of LLMs over simpler models,I'd love to connect with others who use LLMs embeddings and talk in more detail :),Looking to connect with others who use LLMs embeddings.,,,,,,,,,\n",
        "85,Machine Learning Engineer,Machine Learning Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,it isn't in focus on my leaders to use LLMs right now,Management Support,Two key questions I do myself on LLM in prod are: 1 - How is the best strategy to train or fine-tune a LLM ? 2 - How to manage infrastructure costs ?,\"Training Strategy, Infrastructure Costs\",not yet,No,,.,\n",
        "86,CEO,CEO,,,1-10,No,,,,,,,,,,,,,,,,,,,,,,,\"No current need, but also tooling\",\"Lack of Use Case, Lack of Tooling\",\"Reliability, safety, certainty, hallucinations, etc\",\"Reliability, Safety, Certainty, Hallucinations\",No,No,,,\n",
        "87,Principal Cloud Developer,Cloud Developer,Principal,,50-500,Yes,\"text resume, make ppts, text quizzes, text to flash cards, video resume etc,etc\",,\"Text Resume, PowerPoint Generation, Text Quizzes, Text to Flash Cards, Video Resume\",-,,,,-,,\"bad output, costs etc,etc\",\"Bad output,Costs\",Open AI API,-,,Fine tunning LLMS,Fine-Tuning,-,,-,,-,,,,,,,,,,\n",
        "88,CPO,CPO,,,1-10,Yes,We help organizations transition Excel reports to Python,,Code Transcription,\"Yes -- our product, Mito, is a spreadsheet that generates Python code as you edit it. We have integrated the chatgpt api into Mito so users can generate code using the LLM. Then, they can use the Mito spreadsheet to verify that the code it generated is correct.\",Yes,ChatGPT,Code verification,We are not yet sure how large enterprises are going to adopt LLMs. Our large enterprise customers are currently not using the Mito AI feature because there is too much uncertainty.,\"Entreprise Adoption, Uncertainty\",,,Open AI API,Just the Mito spreadsheet,Mito Spreadsheet,Figuring out how to deploy LLMs for enterprise use,Deployment,We give users a spreadsheet interface to verify the output of the LLM. Spreadsheets are great for understanding changes to data!,User Interface,,,,,,,,,,,,,\n",
        "89,Cloud MLOps Engineer,MLOps Engineer,,Cloud,10-50,Yes,Translate french text to english,,Translation,Yes we have used opus-mt model in containerised environment for translation,Yes,Opus-MT,Translation,It is pretty hard to work with this models as they need lot of resources to run which significantly increase the cost also,\"Resource Requirements, Cost\",Need of resources,Infrastructure Requirements,Other model provider API,Docker kubernetes aws resources,\"Docker, Kubernetes, AWS\",Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "90,CEO / Co-Founder,\"CEO, Co-Founder\",,,1-10,Yes,Writing SQL Queries from Natural Language.,,Code Generation,No,No,,,Getting it to write a query was relatively easy. Getting it to write a query that would actually be correct AND execute is much harder.,Accuracy,My concerns at this point are primarily focused around getting the LLM to generate an executable query in all cases.,Correctness,Open AI API,\"Apache Drill, Python.\",\"Apache Drill, Python\",Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "91,Senior MLOps engineer,MLOps Engineer,Senior,,\"1,000+\",Yes,Adverse media about suppliers.,Risk Management,Adverse Media Analysis,\"No, but have plans to do so! I will be involved in the feature store architecture.\",No,,,The unpredictable nature of responses.,Unpredictability,That they give out results that are way off.,Correctness,Other model provider API,\"Databricks, Azure\",\"Databricks, Azure\",Embeddings,Embeddings,Not well :D,,None yet,,,,,,,,,,,,\n",
        "92,machine learning engineer,Machine Learning Engineer,,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Legal said not to,Legal restrictions,How do we keep costs down? How do we validate the outputs?,\"Cost, Output Validation\",nope,No,,,\n",
        "93,CEO,CEO,,,1-10,Yes,Development of production tools for deployment/maintenance of LLMs and other models,\"Software Deployment, Software Maintenance\",,We have a platform to develop/run models in production called Statfish,Yes,Statfish,,Accuracy and hallucinations,\"Accuracy, Hallucinations\",Can't rely on answers 100% of the time,Reliability,\"Open source model (GPT-J, etc)\",\"Jupyter, kubernetes, haystack\",\"Jupyter, Kubernetes, Haystack\",Inferenece for LLMs,Inference,Working on that problem,TODO,,,,,,,,,,,,,\n",
        "94,ML engineer,Machine Learning Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,currently using classic ML and I plan to introduce LLM capabilities to my org,,\"cost of training and inference, time, gpu, ....\",\"Cost, Training, Inference, GPU\",no,No,,,\n",
        "95,Domain Chapter Lead - Data Science and AI,Domain Chapter Leader,Lead,Data Science and AI,\"1,000+\",Yes,Summarisation of customer calls,Customer Support,Text Summarization,nope,No,,,View of accuracy and quality of results,\"Accuracy, Quality\",Reliability,Reliability,Open AI API,Azure stack,Azure Stack,Embeddings,Embeddings,no framework yet,TODO,not much from our side yet,,How we manage quality of LLM outputs ?,How to manage quality of LLM outputs?,,,,,,,,,\n",
        "96,Senior Machine Learning Engineer,Machine Learning Engineer,Senior,,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Lack of applicable business use cases and resources,\"Lack of Use Case, Lack of Resources\",How do you serve them?,Model Serving,Only for a developer,No,,,\n",
        "97,VP of ML,Vice President,,Machine Learning,50-500,No,,,,,,,,,,,,,,,,,,,,,,,\"Expensive, no ROI, complex, dangerous\",\"Cost, Return of Investment, Complex, Dangerous\",Is it reliable? How do we know it's reliable?,Reliability,Yes,Yes,,Expensive and the business use case was poorly defined. Just a shiny toy,\"Cost, Business Requirements\"\n",
        "98,Technical Lead,Technical Leader,,,50-500,Yes,\"POC, evaluation for healthtech\",Healthcare,POC,,,,,supplement/support vs replace,\"Support, Replacement\",user risk,User risk,In house model,hf,Hugging Face,Fine tunning LLMS,Fine-Tuning,cross fingers,Hope,\"not that i can share cheaply, ha\",,\"Growing LLM features iteratively & coherently vs Alphabet's \"\"put LLM in all the things!\"\"\",How to grow LLM features iteratively and coherently?,,,,,,,,,\n",
        "99,Solutions engineer,Solutions Engineer,,,\"500-1,000\",Yes,Summarizes notes from meetings,,Meeting Note Summarization,no.,No,,,Not sure what's the standard,Standardization,The output can be unpredictable,Unpredictability,Open AI API,,,Inferenece for LLMs,Inference,Human evaluation,Human Evaluation,,,,,,,,,,,,,\n",
        "100,Platform Engineer,Platform Engineer,,,50-500,No,,,,,,,,,,,,,,,,,,,,,,,No clear use case for our product,Lack of Use Case,,,No,No,,,\n",
        "101,Senior MLOps,MLOps Engineer,Senior,,\"1,000+\",Yes,Chatbot,,Chatbot,integrate,,,,\"rate limit,\",Rate Limiting,\"privacy, trustfulness, latency\",\"Privacy,Trustfulness,Latency\",Open AI API,vector databases,Vector Databases,Inferenece for LLMs,Inference,eval,Model Evaluation,,,,,,,,,,,,,\n",
        "102,Data science manager,Manager,,Data Science,\"1,000+\",Yes,classification,,Text Classification,no custom tools,No,,,scaling :),Scalability,\"great for batch, terrible for real time\",Real Time Inference,\"Open source model (GPT-J, etc)\",azure ml suite,Azure ML,Embeddings,Embeddings,human verification of sample of inferences,Human Verification,we fine tune a bert sized model with our data and run the inference in batch. We experimented with quantization and distillation and its pretty impressive how much you can do with much smaller models.,Fine-tuned BERT model with quantization and distillation,,,,,,,,,,,\n",
        "103,Data Scientist,Data Scientist,,,\"500-1,000\",No,,,,,,,,,,,,,,,,,,,,,,,LLMs and their hallucinations just are not good for our current use cases (healthcare industry),Hallucinations,,,\"I have not, but I know many of the devs want to; there are some other reasonable use cases too.\",No,,,\n",
        "104,Principal Data Scientist,Data Scientist,Principal,,\"1,000+\",Yes,\"Knowledge graph, transcription, topic modeling, embedding everything\",,\"Knowledge Graph, Text Transcription, Topic Modeling, Embedding\",\"Yeah, but it's super secret until we try and sell it\",Yes,,,Slow. They give bad answers when you set the confidence requirements too high,\"Slowness, Reliability\",Should we be paying for third party providers or train our own. Alpaca anyone?,Build vs buy,Open AI API,\"Nvidia NeMO, Azure OpenAI\",\"Nvidia NeMO, Azure OpenAI\",Embeddings,Embeddings,Feedback button or manual review depending on use case,\"User Feedbacks, Manual Review\",Microsoft's call summarization tool can capture a surprising degree of detail while hilariously completely missing the main point of the meeting,Microsoft's call summarization tool captures detail but misses main point,Best practices to prevent leakage for client facing models that employ online learning?,What are the best practices to prevent leakage for client facing models with online learning?,,,,,,,,,\n",
        "105,Manager Machine learning,Manager,,Machine Learning,\"1,000+\",No,,,,,,,,,,,,,,,,,,,,,,,Less expertise,Lack of Expertise,Large size models latency,Latency,,,,,\n",
        "106,Data scientist,Data Scientist,,,1-10,Yes,User facing chat application in a narrow domain.,,Chatboot,In progress,Yes,,,,,,,Open AI API,,,Inferenece for LLMs,Inference,,,,,,,,,,,,,,,\n",
        "107,Tech Lead,Tech Lead,,,\"1,000+\",Yes,,,,,,,,Compliance,,\"Compliance, security and legal.\",\"Compliance,Security,Legal\",Open AI API,,,Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "108,Sr. Director AI/ML,Director,Senior,AI/ML,\"1,000+\",Yes,\"ITSM, Sales Prospecting\",IT Service Management,Sales Prospecting,chat bot,Yes,,Chatbot,scalabiltiy,Compliance,data privacy,Data privacy,\"Open source model (GPT-J, etc)\",kubeflow,Kubeflow,Fine tunning LLMS,Fine-Tuning,,,,,,,,,,,,,,,\n",
        "109,BDM,Business Development Manager,,,\"1,000+\",Yes,\"code development, summarisation\",,\"Code Development, Text Summarization\",\"development environment, pipelining and deployment tooling\",Yes,,,stack is immature; benchmarks are nonexistent; risk hard/impossible to gauge,Scalability,evaluating efficacy and ROI; understanding power needs/CO2 emissions,\"Evaluation,ROI,Power Consumption,CO2 emissions\",In house model,PyTorch + in-house tools,\"PyTorch, In House\",Inferenece for LLMs,Inference,user feedback,User Feedbacks,\"extremely power-hungry, hard to scale; networking bandwidth is as important as memory when parallelising\",\"LLMs are power-hungry and hard to scale, networking bandwidth is important\",is anyone measuring LLM inference power consumption (and CO2 emissions)? what are the observed ranges of watts/inference of the various model sizes?,Is anyone measuring LLM inference power consumption and CO2 emissions? What are the observed ranges of watts/inference of the various model sizes?,,,,,,,,,'''"
      ],
      "metadata": {
        "id": "ldsM-tYv8rHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTS"
      ],
      "metadata": {
        "id": "D_eKbppz0Jqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgldzxULmxdQ",
        "outputId": "0efd313d-3dd2-4c41-8322-abbbcc1ec7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.io as pio\n",
        "import plotly.express as px \n",
        "\n",
        "from io import StringIO"
      ],
      "metadata": {
        "id": "jqXt40sU0KS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFIGS"
      ],
      "metadata": {
        "id": "Ha6xJz028cD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pio.templates.default = 'plotly_white'"
      ],
      "metadata": {
        "id": "XwDAsPSu8cUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "jMWLO98uOvkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASETS"
      ],
      "metadata": {
        "id": "9E1p5LVc1oWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.read_csv(StringIO(RESULTS), index_col=None)\n",
        "results = results.drop(columns=results.columns[-2:]) # remove duplicates\n",
        "print('Shape:', results.shape)\n",
        "results.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "B-_cBlVH0XYF",
        "outputId": "7a65d310-3f50-4376-9fde-f84378b45ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (110, 17)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  What is your position/title at your company?  \\\n",
              "0                                          NaN   \n",
              "1                               Data scientist   \n",
              "2                                  ML Engineer   \n",
              "3                                      Founder   \n",
              "4                                        Owner   \n",
              "\n",
              "  How big is your organization? (number of employees)  \\\n",
              "0                                                 NaN   \n",
              "1                                              1,000+   \n",
              "2                                              50-500   \n",
              "3                                                1-10   \n",
              "4                                                1-10   \n",
              "\n",
              "  Are you using LLM at in your organization?  \\\n",
              "0                                        NaN   \n",
              "1                                         No   \n",
              "2                                        Yes   \n",
              "3                                        Yes   \n",
              "4                                        Yes   \n",
              "\n",
              "         What is your use case/use cases?  \\\n",
              "0                                     NaN   \n",
              "1                                     NaN   \n",
              "2                      Text summarisation   \n",
              "3  Data annotation; summarization; search   \n",
              "4    Content production and brainstorming   \n",
              "\n",
              "  Have you integrated or built any internal tools to support LLMs in your org? If so what?  \\\n",
              "0                                                                                      NaN   \n",
              "1                                                                                      NaN   \n",
              "2                                                                                       No   \n",
              "3                                    https://chat.mantisnlp.com, internal annotation tools   \n",
              "4                                                                                  Not yet   \n",
              "\n",
              "                     What are some of the main challenges you have encountered thus far when building with LLMs  \\\n",
              "0                                                                                                           NaN   \n",
              "1                                                                                                           NaN   \n",
              "2  Current infrastructure not designed for such massive models, having to implement workarounds for quick fixes   \n",
              "3                         out of date info, hallucinations, cost, difficulty in deploying on own infrastructure   \n",
              "4                                                                                                       not yet   \n",
              "\n",
              "  What are your main concerns with using LLMs in production?  \\\n",
              "0                                                        NaN   \n",
              "1                                                        NaN   \n",
              "2                                                       Cost   \n",
              "3       they could serve nonsense or out of date information   \n",
              "4                                         cost of openai API   \n",
              "\n",
              "          how are you using LLMs?  \\\n",
              "0                             NaN   \n",
              "1                             NaN   \n",
              "2  Open source model (GPT-J, etc)   \n",
              "3                     Open AI API   \n",
              "4                     Open AI API   \n",
              "\n",
              "                      What tools are you using with LLMs?  \\\n",
              "0                                                     NaN   \n",
              "1                                                     NaN   \n",
              "2                                                  Seldon   \n",
              "3                                                     NaN   \n",
              "4  Looking at Pinecone, weaviate, langchain, hugging face   \n",
              "\n",
              "  What areas are you most interested in learning more about?  \\\n",
              "0                                                        NaN   \n",
              "1                                                        NaN   \n",
              "2                                        Inferenece for LLMs   \n",
              "3                                      Working with OSS LLMs   \n",
              "4                                                 Embeddings   \n",
              "\n",
              "                           How do you deal with reliability of output?  \\\n",
              "0                                                                  NaN   \n",
              "1                                                                  NaN   \n",
              "2                                                                  NaN   \n",
              "3  we provide links to the context from which answers have been drawn.   \n",
              "4                                                      temperature = 0   \n",
              "\n",
              "  Any stories you have that are worth sharing about working with LLMs?  \\\n",
              "0                                                                  NaN   \n",
              "1                                                                  NaN   \n",
              "2                                                                  NaN   \n",
              "3                                                                  NaN   \n",
              "4                                                                  NaN   \n",
              "\n",
              "  Any questions you have for the community about LLM in production?  \\\n",
              "0                                                               NaN   \n",
              "1                                                               NaN   \n",
              "2                                                               NaN   \n",
              "3                                                               NaN   \n",
              "4                                                               NaN   \n",
              "\n",
              "  What is the main reason for not using LLMs in your org?  \\\n",
              "0                                                     NaN   \n",
              "1                             Production takes investment   \n",
              "2                                                     NaN   \n",
              "3                                                     NaN   \n",
              "4                                                     NaN   \n",
              "\n",
              "  What are some key questions you face when it comes to using LLM in prod?  \\\n",
              "0                                                                      NaN   \n",
              "1                                             Cost to maintain the service   \n",
              "2                                                                      NaN   \n",
              "3                                                                      NaN   \n",
              "4                                                                      NaN   \n",
              "\n",
              "  have you tried LLMs for different use cases in your org?  \\\n",
              "0                                                      NaN   \n",
              "1                                                      Yes   \n",
              "2                                                      NaN   \n",
              "3                                                      NaN   \n",
              "4                                                      NaN   \n",
              "\n",
              "  If yes, why did it not work out?  \n",
              "0                              NaN  \n",
              "1                     Date quality  \n",
              "2                              NaN  \n",
              "3                              NaN  \n",
              "4                              NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f421101-88a5-4b6c-a97d-1d5185bd14da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>What is your position/title at your company?</th>\n",
              "      <th>How big is your organization? (number of employees)</th>\n",
              "      <th>Are you using LLM at in your organization?</th>\n",
              "      <th>What is your use case/use cases?</th>\n",
              "      <th>Have you integrated or built any internal tools to support LLMs in your org? If so what?</th>\n",
              "      <th>What are some of the main challenges you have encountered thus far when building with LLMs</th>\n",
              "      <th>What are your main concerns with using LLMs in production?</th>\n",
              "      <th>how are you using LLMs?</th>\n",
              "      <th>What tools are you using with LLMs?</th>\n",
              "      <th>What areas are you most interested in learning more about?</th>\n",
              "      <th>How do you deal with reliability of output?</th>\n",
              "      <th>Any stories you have that are worth sharing about working with LLMs?</th>\n",
              "      <th>Any questions you have for the community about LLM in production?</th>\n",
              "      <th>What is the main reason for not using LLMs in your org?</th>\n",
              "      <th>What are some key questions you face when it comes to using LLM in prod?</th>\n",
              "      <th>have you tried LLMs for different use cases in your org?</th>\n",
              "      <th>If yes, why did it not work out?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data scientist</td>\n",
              "      <td>1,000+</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Production takes investment</td>\n",
              "      <td>Cost to maintain the service</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Date quality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ML Engineer</td>\n",
              "      <td>50-500</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Text summarisation</td>\n",
              "      <td>No</td>\n",
              "      <td>Current infrastructure not designed for such massive models, having to implement workarounds for quick fixes</td>\n",
              "      <td>Cost</td>\n",
              "      <td>Open source model (GPT-J, etc)</td>\n",
              "      <td>Seldon</td>\n",
              "      <td>Inferenece for LLMs</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Founder</td>\n",
              "      <td>1-10</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Data annotation; summarization; search</td>\n",
              "      <td>https://chat.mantisnlp.com, internal annotation tools</td>\n",
              "      <td>out of date info, hallucinations, cost, difficulty in deploying on own infrastructure</td>\n",
              "      <td>they could serve nonsense or out of date information</td>\n",
              "      <td>Open AI API</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Working with OSS LLMs</td>\n",
              "      <td>we provide links to the context from which answers have been drawn.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Owner</td>\n",
              "      <td>1-10</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Content production and brainstorming</td>\n",
              "      <td>Not yet</td>\n",
              "      <td>not yet</td>\n",
              "      <td>cost of openai API</td>\n",
              "      <td>Open AI API</td>\n",
              "      <td>Looking at Pinecone, weaviate, langchain, hugging face</td>\n",
              "      <td>Embeddings</td>\n",
              "      <td>temperature = 0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f421101-88a5-4b6c-a97d-1d5185bd14da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f421101-88a5-4b6c-a97d-1d5185bd14da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f421101-88a5-4b6c-a97d-1d5185bd14da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "changes = pd.read_csv(StringIO(CHANGES), index_col=0)\n",
        "changes = changes.drop(columns=results.columns)\n",
        "print('Shape:', changes.shape)\n",
        "changes.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "rWrBJ33s90f-",
        "outputId": "c5290136-1779-4e41-e5d1-cf824c06e387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (110, 20)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       title position other            fields  \\\n",
              "0                        NaN      NaN   NaN               NaN   \n",
              "1             Data Scientist      NaN   NaN               NaN   \n",
              "2  Machine Learning Engineer      NaN   NaN               NaN   \n",
              "3                    Founder      NaN   NaN               NaN   \n",
              "4                      Owner      NaN   NaN  Content Creation   \n",
              "\n",
              "                                         tasks integrated  \\\n",
              "0                                          NaN        NaN   \n",
              "1                                          NaN        NaN   \n",
              "2                           Text Summarization         No   \n",
              "3  Data Annotation, Text Summarization, Search         No   \n",
              "4                                          NaN         No   \n",
              "\n",
              "                              solutions purposes  \\\n",
              "0                                   NaN      NaN   \n",
              "1                                   NaN      NaN   \n",
              "2                                   NaN      NaN   \n",
              "3  MantisNLP, internal annotation tools      NaN   \n",
              "4                                   NaN      NaN   \n",
              "\n",
              "                                      challenges            concerns  \\\n",
              "0                                            NaN                 NaN   \n",
              "1                                            NaN                 NaN   \n",
              "2                     Infrastructure,Workarounds                Cost   \n",
              "3  Accuracy,Outdated Information,Cost,Deployment  Accuracy, Currency   \n",
              "4                                            NaN                Cost   \n",
              "\n",
              "                                         tools       topics  \\\n",
              "0                                          NaN          NaN   \n",
              "1                                          NaN          NaN   \n",
              "2                                       Seldon    Inference   \n",
              "3                                          NaN  Open Source   \n",
              "4  Pinecone, Weaviate, LangChain, Hugging Face   Embeddings   \n",
              "\n",
              "            approaches stories questions          reasons  \\\n",
              "0                  NaN     NaN       NaN              NaN   \n",
              "1                  NaN     NaN       NaN  Production Cost   \n",
              "2                  NaN     NaN       NaN              NaN   \n",
              "3              Nothing     NaN       NaN              NaN   \n",
              "4  Temperature Control     NaN       NaN              NaN   \n",
              "\n",
              "          challenges.1 tried cases      problems  \n",
              "0                  NaN   NaN   NaN           NaN  \n",
              "1  Cost of Maintenance   Yes   NaN  Data Quality  \n",
              "2                  NaN   NaN   NaN           NaN  \n",
              "3                  NaN   NaN   NaN           NaN  \n",
              "4                  NaN   NaN   NaN           NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09dfca4c-bb66-4c94-a146-0b5a38a0b440\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>position</th>\n",
              "      <th>other</th>\n",
              "      <th>fields</th>\n",
              "      <th>tasks</th>\n",
              "      <th>integrated</th>\n",
              "      <th>solutions</th>\n",
              "      <th>purposes</th>\n",
              "      <th>challenges</th>\n",
              "      <th>concerns</th>\n",
              "      <th>tools</th>\n",
              "      <th>topics</th>\n",
              "      <th>approaches</th>\n",
              "      <th>stories</th>\n",
              "      <th>questions</th>\n",
              "      <th>reasons</th>\n",
              "      <th>challenges.1</th>\n",
              "      <th>tried</th>\n",
              "      <th>cases</th>\n",
              "      <th>problems</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Production Cost</td>\n",
              "      <td>Cost of Maintenance</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Data Quality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Machine Learning Engineer</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Text Summarization</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Infrastructure,Workarounds</td>\n",
              "      <td>Cost</td>\n",
              "      <td>Seldon</td>\n",
              "      <td>Inference</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Founder</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Data Annotation, Text Summarization, Search</td>\n",
              "      <td>No</td>\n",
              "      <td>MantisNLP, internal annotation tools</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Accuracy,Outdated Information,Cost,Deployment</td>\n",
              "      <td>Accuracy, Currency</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Open Source</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Owner</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Content Creation</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cost</td>\n",
              "      <td>Pinecone, Weaviate, LangChain, Hugging Face</td>\n",
              "      <td>Embeddings</td>\n",
              "      <td>Temperature Control</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09dfca4c-bb66-4c94-a146-0b5a38a0b440')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09dfca4c-bb66-4c94-a146-0b5a38a0b440 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09dfca4c-bb66-4c94-a146-0b5a38a0b440');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = pd.read_csv(StringIO(REVIEWS), index_col=0)\n",
        "reviews = reviews.drop(columns=results.columns)\n",
        "print('Shape:', reviews.shape)\n",
        "reviews.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "NH0oRPCe1-mp",
        "outputId": "1985a027-38a4-4312-dcd2-4823c3ab4980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (110, 20)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       title position other fields  \\\n",
              "0                        NaN      NaN   NaN    NaN   \n",
              "1             Data Scientist      NaN   NaN    NaN   \n",
              "2  Machine Learning Engineer      NaN   NaN    NaN   \n",
              "3                    Founder      NaN   NaN    NaN   \n",
              "4                      Owner      NaN   NaN    NaN   \n",
              "\n",
              "                                         tasks integrated  solutions  \\\n",
              "0                                          NaN        NaN        NaN   \n",
              "1                                          NaN        NaN        NaN   \n",
              "2                           Text Summarization         No        NaN   \n",
              "3  Data Annotation, Text Summarization, Search         No  MantisNLP   \n",
              "4              Content Creation, Brainstorming         No        NaN   \n",
              "\n",
              "            purposes                                     challenges  \\\n",
              "0                NaN                                            NaN   \n",
              "1                NaN                                            NaN   \n",
              "2                NaN                     Infrastructure,Workarounds   \n",
              "3  Internal Solution  Accuracy,Outdated Information,Cost,Deployment   \n",
              "4                NaN                                            NaN   \n",
              "\n",
              "                         concerns  \\\n",
              "0                             NaN   \n",
              "1                             NaN   \n",
              "2                            Cost   \n",
              "3  Accuracy, Outdated Information   \n",
              "4                            Cost   \n",
              "\n",
              "                                         tools       topics  \\\n",
              "0                                          NaN          NaN   \n",
              "1                                          NaN          NaN   \n",
              "2                                       Seldon    Inference   \n",
              "3                                          NaN  Open Source   \n",
              "4  Pinecone, Weaviate, LangChain, Hugging Face   Embeddings   \n",
              "\n",
              "            approaches stories questions          reasons  \\\n",
              "0                  NaN     NaN       NaN              NaN   \n",
              "1                  NaN     NaN       NaN  Production Cost   \n",
              "2                  NaN     NaN       NaN              NaN   \n",
              "3           References     NaN       NaN              NaN   \n",
              "4  Temperature Control     NaN       NaN              NaN   \n",
              "\n",
              "          challenges.1 tried cases      problems  \n",
              "0                  NaN   NaN   NaN           NaN  \n",
              "1  Cost of Maintenance   Yes   NaN  Data Quality  \n",
              "2                  NaN   NaN   NaN           NaN  \n",
              "3                  NaN   NaN   NaN           NaN  \n",
              "4                  NaN   NaN   NaN           NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c70bb12-202f-4047-adcc-c1c3986a4dee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>position</th>\n",
              "      <th>other</th>\n",
              "      <th>fields</th>\n",
              "      <th>tasks</th>\n",
              "      <th>integrated</th>\n",
              "      <th>solutions</th>\n",
              "      <th>purposes</th>\n",
              "      <th>challenges</th>\n",
              "      <th>concerns</th>\n",
              "      <th>tools</th>\n",
              "      <th>topics</th>\n",
              "      <th>approaches</th>\n",
              "      <th>stories</th>\n",
              "      <th>questions</th>\n",
              "      <th>reasons</th>\n",
              "      <th>challenges.1</th>\n",
              "      <th>tried</th>\n",
              "      <th>cases</th>\n",
              "      <th>problems</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Production Cost</td>\n",
              "      <td>Cost of Maintenance</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Data Quality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Machine Learning Engineer</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Text Summarization</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Infrastructure,Workarounds</td>\n",
              "      <td>Cost</td>\n",
              "      <td>Seldon</td>\n",
              "      <td>Inference</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Founder</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Data Annotation, Text Summarization, Search</td>\n",
              "      <td>No</td>\n",
              "      <td>MantisNLP</td>\n",
              "      <td>Internal Solution</td>\n",
              "      <td>Accuracy,Outdated Information,Cost,Deployment</td>\n",
              "      <td>Accuracy, Outdated Information</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Open Source</td>\n",
              "      <td>References</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Owner</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Content Creation, Brainstorming</td>\n",
              "      <td>No</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cost</td>\n",
              "      <td>Pinecone, Weaviate, LangChain, Hugging Face</td>\n",
              "      <td>Embeddings</td>\n",
              "      <td>Temperature Control</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c70bb12-202f-4047-adcc-c1c3986a4dee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0c70bb12-202f-4047-adcc-c1c3986a4dee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0c70bb12-202f-4047-adcc-c1c3986a4dee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSIS"
      ],
      "metadata": {
        "id": "ZQdpEdXf9cgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "miss = (reviews.notna() & changes.isna()) | (reviews.isna() & changes.notna())\n",
        "miss.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "7NpK0wC6evyt",
        "outputId": "395d2a59-2306-43bd-a205-44ff5ccd8c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   title  position  other  fields  tasks  integrated  solutions  purposes  \\\n",
              "0  False     False  False   False  False       False      False     False   \n",
              "1  False     False  False   False  False       False      False     False   \n",
              "2  False     False  False   False  False       False      False     False   \n",
              "3  False     False  False   False  False       False      False      True   \n",
              "4  False     False  False    True   True       False      False     False   \n",
              "\n",
              "   challenges  concerns  tools  topics  approaches  stories  questions  \\\n",
              "0       False     False  False   False       False    False      False   \n",
              "1       False     False  False   False       False    False      False   \n",
              "2       False     False  False   False       False    False      False   \n",
              "3       False     False  False   False       False    False      False   \n",
              "4       False     False  False   False       False    False      False   \n",
              "\n",
              "   reasons  challenges.1  tried  cases  problems  \n",
              "0    False         False  False  False     False  \n",
              "1    False         False  False  False     False  \n",
              "2    False         False  False  False     False  \n",
              "3    False         False  False  False     False  \n",
              "4    False         False  False  False     False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd24738a-0106-4edb-89e2-537336bb55a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>position</th>\n",
              "      <th>other</th>\n",
              "      <th>fields</th>\n",
              "      <th>tasks</th>\n",
              "      <th>integrated</th>\n",
              "      <th>solutions</th>\n",
              "      <th>purposes</th>\n",
              "      <th>challenges</th>\n",
              "      <th>concerns</th>\n",
              "      <th>tools</th>\n",
              "      <th>topics</th>\n",
              "      <th>approaches</th>\n",
              "      <th>stories</th>\n",
              "      <th>questions</th>\n",
              "      <th>reasons</th>\n",
              "      <th>challenges.1</th>\n",
              "      <th>tried</th>\n",
              "      <th>cases</th>\n",
              "      <th>problems</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd24738a-0106-4edb-89e2-537336bb55a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd24738a-0106-4edb-89e2-537336bb55a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd24738a-0106-4edb-89e2-537336bb55a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff = reviews.ne(changes) & reviews.notna() & changes.notna()\n",
        "diff.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "kc7xj8_kb44b",
        "outputId": "0019c999-df2b-4a22-cbbe-7532cd777c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   title  position  other  fields  tasks  integrated  solutions  purposes  \\\n",
              "0  False     False  False   False  False       False      False     False   \n",
              "1  False     False  False   False  False       False      False     False   \n",
              "2  False     False  False   False  False       False      False     False   \n",
              "3  False     False  False   False  False       False       True     False   \n",
              "4  False     False  False   False  False       False      False     False   \n",
              "\n",
              "   challenges  concerns  tools  topics  approaches  stories  questions  \\\n",
              "0       False     False  False   False       False    False      False   \n",
              "1       False     False  False   False       False    False      False   \n",
              "2       False     False  False   False       False    False      False   \n",
              "3       False      True  False   False        True    False      False   \n",
              "4       False     False  False   False       False    False      False   \n",
              "\n",
              "   reasons  challenges.1  tried  cases  problems  \n",
              "0    False         False  False  False     False  \n",
              "1    False         False  False  False     False  \n",
              "2    False         False  False  False     False  \n",
              "3    False         False  False  False     False  \n",
              "4    False         False  False  False     False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75c3d0e6-f98b-418f-a36b-10f9c1d502b9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>position</th>\n",
              "      <th>other</th>\n",
              "      <th>fields</th>\n",
              "      <th>tasks</th>\n",
              "      <th>integrated</th>\n",
              "      <th>solutions</th>\n",
              "      <th>purposes</th>\n",
              "      <th>challenges</th>\n",
              "      <th>concerns</th>\n",
              "      <th>tools</th>\n",
              "      <th>topics</th>\n",
              "      <th>approaches</th>\n",
              "      <th>stories</th>\n",
              "      <th>questions</th>\n",
              "      <th>reasons</th>\n",
              "      <th>challenges.1</th>\n",
              "      <th>tried</th>\n",
              "      <th>cases</th>\n",
              "      <th>problems</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75c3d0e6-f98b-418f-a36b-10f9c1d502b9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-75c3d0e6-f98b-418f-a36b-10f9c1d502b9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-75c3d0e6-f98b-418f-a36b-10f9c1d502b9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "same = reviews.eq(changes)\n",
        "same.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "JjUAvsTSW4vw",
        "outputId": "ce4ad63a-5379-4a87-e91f-38a11aeb7017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   title  position  other  fields  tasks  integrated  solutions  purposes  \\\n",
              "0  False     False  False   False  False       False      False     False   \n",
              "1   True     False  False   False  False       False      False     False   \n",
              "2   True     False  False   False   True        True      False     False   \n",
              "3   True     False  False   False   True        True      False     False   \n",
              "4   True     False  False   False  False        True      False     False   \n",
              "\n",
              "   challenges  concerns  tools  topics  approaches  stories  questions  \\\n",
              "0       False     False  False   False       False    False      False   \n",
              "1       False     False  False   False       False    False      False   \n",
              "2        True      True   True    True       False    False      False   \n",
              "3        True     False  False    True       False    False      False   \n",
              "4       False      True   True    True        True    False      False   \n",
              "\n",
              "   reasons  challenges.1  tried  cases  problems  \n",
              "0    False         False  False  False     False  \n",
              "1     True          True   True  False      True  \n",
              "2    False         False  False  False     False  \n",
              "3    False         False  False  False     False  \n",
              "4    False         False  False  False     False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d34d2b09-dd54-42c9-bc57-d132862a8e87\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>position</th>\n",
              "      <th>other</th>\n",
              "      <th>fields</th>\n",
              "      <th>tasks</th>\n",
              "      <th>integrated</th>\n",
              "      <th>solutions</th>\n",
              "      <th>purposes</th>\n",
              "      <th>challenges</th>\n",
              "      <th>concerns</th>\n",
              "      <th>tools</th>\n",
              "      <th>topics</th>\n",
              "      <th>approaches</th>\n",
              "      <th>stories</th>\n",
              "      <th>questions</th>\n",
              "      <th>reasons</th>\n",
              "      <th>challenges.1</th>\n",
              "      <th>tried</th>\n",
              "      <th>cases</th>\n",
              "      <th>problems</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d34d2b09-dd54-42c9-bc57-d132862a8e87')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d34d2b09-dd54-42c9-bc57-d132862a8e87 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d34d2b09-dd54-42c9-bc57-d132862a8e87');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analysis = (\n",
        "    reviews\n",
        "    .mask(same, other=1)\n",
        "    .mask(diff, other=-1)\n",
        "    .mask(miss, other=-1)\n",
        "    .fillna(0)\n",
        ")\n",
        "analysis.head()"
      ],
      "metadata": {
        "id": "6gJu7QRl6ECK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "0b2cf4de-095b-4dec-fa17-eb3d0814a06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   title  position  other  fields  tasks  integrated  solutions  purposes  \\\n",
              "0      0         0      0       0      0           0          0         0   \n",
              "1      1         0      0       0      0           0          0         0   \n",
              "2      1         0      0       0      1           1          0         0   \n",
              "3      1         0      0       0      1           1         -1        -1   \n",
              "4      1         0      0      -1     -1           1          0         0   \n",
              "\n",
              "   challenges  concerns  tools  topics  approaches  stories  questions  \\\n",
              "0           0         0      0       0           0        0          0   \n",
              "1           0         0      0       0           0        0          0   \n",
              "2           1         1      1       1           0        0          0   \n",
              "3           1        -1      0       1          -1        0          0   \n",
              "4           0         1      1       1           1        0          0   \n",
              "\n",
              "   reasons  challenges.1  tried  cases  problems  \n",
              "0        0             0      0      0         0  \n",
              "1        1             1      1      0         1  \n",
              "2        0             0      0      0         0  \n",
              "3        0             0      0      0         0  \n",
              "4        0             0      0      0         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f50e6f6-cd88-4b27-b26b-6df568b4b311\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>position</th>\n",
              "      <th>other</th>\n",
              "      <th>fields</th>\n",
              "      <th>tasks</th>\n",
              "      <th>integrated</th>\n",
              "      <th>solutions</th>\n",
              "      <th>purposes</th>\n",
              "      <th>challenges</th>\n",
              "      <th>concerns</th>\n",
              "      <th>tools</th>\n",
              "      <th>topics</th>\n",
              "      <th>approaches</th>\n",
              "      <th>stories</th>\n",
              "      <th>questions</th>\n",
              "      <th>reasons</th>\n",
              "      <th>challenges.1</th>\n",
              "      <th>tried</th>\n",
              "      <th>cases</th>\n",
              "      <th>problems</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f50e6f6-cd88-4b27-b26b-6df568b4b311')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7f50e6f6-cd88-4b27-b26b-6df568b4b311 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7f50e6f6-cd88-4b27-b26b-6df568b4b311');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# both answers per question\n",
        "both = analysis.applymap(lambda x: x != 0).sum()\n",
        "print('Total:', both.sum())\n",
        "both"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzTVeVkQlPaF",
        "outputId": "ee70a060-759e-4140-9fcc-ea349ffe9b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: 803\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "title           94\n",
              "position        52\n",
              "other           25\n",
              "fields          23\n",
              "tasks           57\n",
              "integrated      56\n",
              "solutions       28\n",
              "purposes        32\n",
              "challenges      58\n",
              "concerns        58\n",
              "tools           46\n",
              "topics          67\n",
              "approaches      43\n",
              "stories         17\n",
              "questions       20\n",
              "reasons         41\n",
              "challenges.1    31\n",
              "tried           31\n",
              "cases            9\n",
              "problems        15\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# good answers per question\n",
        "goods = analysis.applymap(lambda x: x > 0).sum()\n",
        "print('Total:', goods.sum())\n",
        "goods"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GJ3M7QYjPkD",
        "outputId": "383b45f1-e2d3-4fcd-c35d-4cb57ecd96bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: 514\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "title           59\n",
              "position        27\n",
              "other           14\n",
              "fields          18\n",
              "tasks           44\n",
              "integrated      38\n",
              "solutions        6\n",
              "purposes        10\n",
              "challenges      26\n",
              "concerns        37\n",
              "tools           39\n",
              "topics          63\n",
              "approaches      31\n",
              "stories         12\n",
              "questions       18\n",
              "reasons         12\n",
              "challenges.1    28\n",
              "tried           23\n",
              "cases            3\n",
              "problems         6\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bad answers per question\n",
        "bads = analysis.applymap(lambda x: x < 0).sum()\n",
        "print('Total:', bads.sum())\n",
        "bads"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ollIc4Rmk-Yd",
        "outputId": "7c588683-521c-40bd-ebb9-db1a77959a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: 289\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "title           35\n",
              "position        25\n",
              "other           11\n",
              "fields           5\n",
              "tasks           13\n",
              "integrated      18\n",
              "solutions       22\n",
              "purposes        22\n",
              "challenges      32\n",
              "concerns        21\n",
              "tools            7\n",
              "topics           4\n",
              "approaches      12\n",
              "stories          5\n",
              "questions        2\n",
              "reasons         29\n",
              "challenges.1     3\n",
              "tried            8\n",
              "cases            6\n",
              "problems         9\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats = pd.concat([goods, bads], axis='columns').rename(columns={0: 'Good', 1: 'Bad'})\n",
        "stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "BMEueo53lolX",
        "outputId": "40972bb3-f0e6-48fe-ae6c-4fcc06a630c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Good  Bad\n",
              "title           59   35\n",
              "position        27   25\n",
              "other           14   11\n",
              "fields          18    5\n",
              "tasks           44   13\n",
              "integrated      38   18\n",
              "solutions        6   22\n",
              "purposes        10   22\n",
              "challenges      26   32\n",
              "concerns        37   21\n",
              "tools           39    7\n",
              "topics          63    4\n",
              "approaches      31   12\n",
              "stories         12    5\n",
              "questions       18    2\n",
              "reasons         12   29\n",
              "challenges.1    28    3\n",
              "tried           23    8\n",
              "cases            3    6\n",
              "problems         6    9"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96a40c72-42db-4098-9f1b-30e373fa71d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Good</th>\n",
              "      <th>Bad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>title</th>\n",
              "      <td>59</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>position</th>\n",
              "      <td>27</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other</th>\n",
              "      <td>14</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fields</th>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tasks</th>\n",
              "      <td>44</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>integrated</th>\n",
              "      <td>38</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>solutions</th>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>purposes</th>\n",
              "      <td>10</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>challenges</th>\n",
              "      <td>26</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concerns</th>\n",
              "      <td>37</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tools</th>\n",
              "      <td>39</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>topics</th>\n",
              "      <td>63</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approaches</th>\n",
              "      <td>31</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stories</th>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>questions</th>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reasons</th>\n",
              "      <td>12</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>challenges.1</th>\n",
              "      <td>28</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tried</th>\n",
              "      <td>23</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cases</th>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>problems</th>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96a40c72-42db-4098-9f1b-30e373fa71d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-96a40c72-42db-4098-9f1b-30e373fa71d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-96a40c72-42db-4098-9f1b-30e373fa71d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(stats, title='Number of good/bad answers per question')\n",
        "fig.write_image('questions.png')\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "NNmMMPxelkfv",
        "outputId": "22a4d348-66f2-4b96-85b8-60b324dc57fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"c2f00a66-ef49-47d6-8f25-d82a04f5a279\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c2f00a66-ef49-47d6-8f25-d82a04f5a279\")) {                    Plotly.newPlot(                        \"c2f00a66-ef49-47d6-8f25-d82a04f5a279\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"variable=Good<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Good\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Good\",\"offsetgroup\":\"Good\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"title\",\"position\",\"other\",\"fields\",\"tasks\",\"integrated\",\"solutions\",\"purposes\",\"challenges\",\"concerns\",\"tools\",\"topics\",\"approaches\",\"stories\",\"questions\",\"reasons\",\"challenges.1\",\"tried\",\"cases\",\"problems\"],\"xaxis\":\"x\",\"y\":[59,27,14,18,44,38,6,10,26,37,39,63,31,12,18,12,28,23,3,6],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"variable=Bad<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Bad\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Bad\",\"offsetgroup\":\"Bad\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"title\",\"position\",\"other\",\"fields\",\"tasks\",\"integrated\",\"solutions\",\"purposes\",\"challenges\",\"concerns\",\"tools\",\"topics\",\"approaches\",\"stories\",\"questions\",\"reasons\",\"challenges.1\",\"tried\",\"cases\",\"problems\"],\"xaxis\":\"x\",\"y\":[35,25,11,5,13,18,22,22,32,21,7,4,12,5,2,29,3,8,6,9],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"bgcolor\":\"white\",\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"white\",\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"white\",\"subunitcolor\":\"#C8D4E3\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Number of good/bad answers per question\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c2f00a66-ef49-47d6-8f25-d82a04f5a279');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(stats.sum(), title='Number of good/bad answers', width=700)\n",
        "fig.update_layout(showlegend=False)\n",
        "fig.write_image('total.png')\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "P_CE_SXMmBLO",
        "outputId": "2b009cb2-3392-401b-f740-8751b0d49e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"a954310b-8bf4-476f-b93b-2934e5034ffb\" class=\"plotly-graph-div\" style=\"height:525px; width:700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a954310b-8bf4-476f-b93b-2934e5034ffb\")) {                    Plotly.newPlot(                        \"a954310b-8bf4-476f-b93b-2934e5034ffb\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"Good\",\"Bad\"],\"xaxis\":\"x\",\"y\":[514,289],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"bgcolor\":\"white\",\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"white\",\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"white\",\"subunitcolor\":\"#C8D4E3\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Number of good/bad answers\"},\"barmode\":\"relative\",\"width\":700,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a954310b-8bf4-476f-b93b-2934e5034ffb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}